from neo4j import GraphDatabase
import json
import time
from datetime import datetime


from neo4j_connection import get_neo4j_driver

class VulnerabilityRepoMapper:
    def __init__(self, batch_size=5000):
        self._driver = None
        self.batch_size = batch_size  # Number of records to process in each batch
        
    def connect(self):
        self._driver = get_neo4j_driver()
        return self._driver is not None
        
    def close(self):
        """Close the Neo4j connection if open"""
        if self._driver:
            self._driver.close()
            print("Neo4j connection closed.")
    
    def get_vulnerability_count(self):
        """Get the total count of vulnerabilities in the database for a specific repo"""
        if not self._driver:
            print("Error: Not connected to Neo4j. Call connect() first.")
            return 0
            
        with self._driver.session() as session:
            query = """
            MATCH (v:Vulnerability)-[:BELONGS_TO]->(vr:VULN_REPO)
            MATCH (v)-[:AFFECTS]->(p:Package)
            WHERE vr.name = 'OSV'
            RETURN COUNT(*) AS count
            """
            result = session.run(query)
            record = result.single()
            return record["count"] if record else 0
    
    def get_vulnerability_repo_mapping_batched(self, repo_name="OSV", progress_interval=10000):
        """
        Create a nested dictionary mapping vuln repositories to their vulnerabilities and affected versions
        """
        if not self._driver:
            print("Error: Not connected to Neo4j. Call connect() first.")
            return {}
            
        total_count = self.get_vulnerability_count()
        print(f"Processing {total_count} vulnerabilities for repo '{repo_name}'...")
        
        vuln_repo_map = {}
        processed_count = 0
        start_time = time.time()
        
        with self._driver.session() as session:
            skip = 0
            
            while True:
                query = f"""
                MATCH (v:Vulnerability)-[:BELONGS_TO]->(vr:VULN_REPO)
                MATCH (v)-[:AFFECTS]->(p:Package)
                WHERE vr.name = $repo_name
                RETURN vr.name AS repo_name, v.id AS vuln_id, p.versions AS affected_versions
                SKIP {skip} LIMIT {self.batch_size}
                """
                
                # Execute the query for this batch
                results = list(session.run(query, {"repo_name": repo_name}))
                
                # If no more results, we're done
                if not results:
                    break
                
                # Process this batch of results
                for record in results:
                    repo_name = record['repo_name']
                    vuln_id = record['vuln_id']
                    affected_versions = record['affected_versions']
                    
                    # Create repo entry if it doesnt exist
                    if repo_name not in vuln_repo_map:
                        vuln_repo_map[repo_name] = {}
                    
                    # Create vulnerability entry if it doesnt exist
                    if vuln_id not in vuln_repo_map[repo_name]:
                        vuln_repo_map[repo_name][vuln_id] = []
                    
                    # Handle affected_versions based on whether it's a list or a single value
                    if isinstance(affected_versions, list):
                        # Add each version if it isnt present
                        for version in affected_versions:
                            if version not in vuln_repo_map[repo_name][vuln_id]:
                                vuln_repo_map[repo_name][vuln_id].append(version)
                    else:
                        # Add the single version if it isnt present
                        if affected_versions not in vuln_repo_map[repo_name][vuln_id]:
                            vuln_repo_map[repo_name][vuln_id].append(affected_versions)
                
                batch_size = len(results)
                processed_count += batch_size
                
                if processed_count % progress_interval < self.batch_size:
                    elapsed = time.time() - start_time
                    percent = (processed_count / total_count) * 100 if total_count > 0 else 0
                    records_per_second = processed_count / elapsed if elapsed > 0 else 0
                    eta_seconds = (total_count - processed_count) / records_per_second if records_per_second > 0 else 0
                    
                    print(f"Progress: {processed_count}/{total_count} ({percent:.1f}%) - "
                          f"Speed: {records_per_second:.1f} records/sec - "
                          f"ETA: {datetime.fromtimestamp(time.time() + eta_seconds).strftime('%H:%M:%S')}")
                
                # Prepare for next batch
                skip += batch_size
                
                # If we got fewer results than batch_size, we're at the end
                if batch_size < self.batch_size:
                    break
        
        print(f"Completed processing {processed_count} records in {time.time() - start_time:.1f} seconds")
        print(f"Found {sum(len(vulns) for vulns in vuln_repo_map.values())} unique vulnerabilities")
        
        return vuln_repo_map
    
    def export_to_json(self, repo_name="OSV", filename=None):
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f'vulnerability_repo_map_{repo_name}_{timestamp}.json'
        
        print(f"Starting export to {filename}...")
        start_time = time.time()
        
        vuln_repo_map = self.get_vulnerability_repo_mapping_batched(repo_name)
        
        if not vuln_repo_map:
            print("No data to export.")
            return False
        
        # Write to file
        print(f"Writing data to {filename}...")
        with open(filename, 'w') as f:
            json.dump(vuln_repo_map, f, indent=2)
        
        print(f"Export completed in {time.time() - start_time:.1f} seconds")
        print(f"Exported vulnerability repo mapping to {filename}")
        return True

    def export_to_json_streaming(self, filename=None, progress_interval=10000):
        if not self._driver:
            print("Error: Not connected to Neo4j. Call connect() first.")
            return False

        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f'package_cve_versions_{timestamp}.json'

        print(f"Starting streaming export to {filename}...")
        start_time = time.time()

        total_count = self.get_vulnerability_count()
        print(f"Processing {total_count} vulnerability relationships...")

        processed_count = 0
        package_count = 0

        with open(filename, 'w') as f:
            f.write("{\n")  # Start the JSON object

            is_first_package = True

            with self._driver.session() as session:
                query = """
                    MATCH (v:Vulnerability)-[:AFFECTS]->(p:Package)
                    RETURN p.name AS package_name, p.ecosystem AS ecosystem, v.id AS vuln_id, collect(p.versions) AS affected_versions
                    ORDER BY p.name, v.id
                """

                result = session.run(query)

                current_package = None
                is_first_vuln = True

                for record in result:
                    package_name = record['package_name']
                    ecosystem = record['ecosystem']
                    vuln_id = record['vuln_id']
                    all_versions = record['affected_versions']

                    affected_versions = []
                    for version_item in all_versions:
                        if isinstance(version_item, list):
                            affected_versions.extend(version_item)
                        else:
                            affected_versions.append(version_item)

                    unique_versions = list(set(affected_versions))

                    if package_name != current_package:
                        if current_package is not None:
                            f.write("\n        },\n")  # Close the previous package object

                        if not is_first_package:
                            f.write(",\n")  # add comma between packages

                        f.write(f'        "{package_name}": {{\n')  # Start a new package object
                        f.write(f'            "ecosystem": "{ecosystem}",\n')
                        current_package = package_name
                        is_first_vuln = True
                        package_count += 1
                        is_first_package = False

                    if not is_first_vuln:
                        f.write(",\n")  # Add comma between vulnerabilities

                    f.write(f'            "{vuln_id}": {json.dumps(unique_versions, indent=12)}')
                    is_first_vuln = False
                    processed_count += 1

                    if processed_count % progress_interval == 0:
                        elapsed = time.time() - start_time
                        percent = (processed_count / total_count) * 100 if total_count > 0 else 0
                        records_per_second = processed_count / elapsed if elapsed > 0 else 0
                        eta_seconds = (total_count - processed_count) / records_per_second if records_per_second > 0 else 0

                        print(f"Progress: {processed_count}/{total_count} ({percent:.1f}%) - "
                              f"Packages: {package_count} - "
                              f"Speed: {records_per_second:.1f} records/sec - "
                              f"ETA: {datetime.fromtimestamp(time.time() + eta_seconds).strftime('%H:%M:%S')}")

                if current_package is not None:
                    f.write("\n        }\n")

                f.write("}\n")

        print(f"Completed processing {processed_count} records in {time.time() - start_time:.1f} seconds")
        print(f"Found {package_count} unique packages")
        print(f"Exported package CVE versions to {filename}")
        return True

def main():
    mapper = VulnerabilityRepoMapper(batch_size=10000)
    
    try:
        if mapper.connect():
            print("Successfully connected to Neo4j database.")
            
            total_vulns = mapper.get_vulnerability_count()
            print(f"Found {total_vulns} vulnerability relationships in the database for repo 'OSV'.")
            
            if total_vulns > 100000:
                print("Large dataset detected. Using memory-efficient streaming export...")
                mapper.export_to_json_streaming()
            else:
                print("Using standard batch processing...")
                mapper.export_to_json()
        else:
            print("Failed to connect to Neo4j database.")
    
    except Exception as e:
        print(f"An error occurred: {e}")
    
    finally:
        mapper.close()


if __name__ == "__main__":
    main()