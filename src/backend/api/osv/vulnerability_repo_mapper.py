from neo4j import GraphDatabase
import json
import time
from datetime import datetime
import logging
from osv.neo4j_connection import get_neo4j_driver
import math
from ortools.sat.python import cp_model
import re

class VulnerabilityRepoMapper:
    def __init__(self, batch_size=5000):
        self._driver = None
        self.batch_size = batch_size  # Number of records to process in each batch

    def connect(self):
        self._driver = get_neo4j_driver()
        return self._driver is not None

    def close(self):
        """Close the Neo4j connection if open"""
        if self._driver:
            self._driver.close()
            print("Neo4j connection closed.")

    def get_vulnerability_count(self):
        """Get the total count of vulnerabilities in the database for a specific repo"""
        if not self._driver:
            print("Error: Not connected to Neo4j. Call connect() first.")
            return 0

        with self._driver.session() as session:
            query = """
            MATCH (v:Vulnerability)-[:BELONGS_TO]->(vr:VULN_REPO)
            MATCH (v)-[:AFFECTS]->(p:Package)
            WHERE vr.name = 'OSV'
            RETURN COUNT(*) AS count
            """
            result = session.run(query)
            record = result.single()
            return record["count"] if record else 0

    def get_vulnerability_repo_mapping_batched(self, repo_name="OSV", progress_interval=10000):
        """
        Create a nested dictionary mapping vuln repositories to their vulnerabilities and affected versions.

        Output structure:
        {
          "OSV": {
            "CVE-1234": ["v1.0", "v1.2"],
            "CVE-5678": ["v1.1", "v1.3"],
            ...
          }
        }
        """
        if not self._driver:
            print("Error: Not connected to Neo4j. Call connect() first.")
            return {}

        total_count = self.get_vulnerability_count()
        print(f"Processing {total_count} vulnerabilities for repo '{repo_name}'...")

        vuln_repo_map = {}
        processed_count = 0
        start_time = time.time()

        with self._driver.session() as session:
            skip = 0

            while True:
                query = f"""
                MATCH (v:Vulnerability)-[:BELONGS_TO]->(vr:VULN_REPO)
                MATCH (v)-[:AFFECTS]->(p:Package)
                WHERE vr.name = $repo_name
                RETURN vr.name AS repo_name, v.id AS vuln_id, p.versions AS affected_versions
                SKIP {skip} LIMIT {self.batch_size}
                """

                results = list(session.run(query, {"repo_name": repo_name}))
                if not results:
                    break

                for record in results:
                    repo_name_db = record['repo_name']
                    vuln_id = record['vuln_id']
                    affected_versions = record['affected_versions']

                    if repo_name_db not in vuln_repo_map:
                        vuln_repo_map[repo_name_db] = {}

                    if vuln_id not in vuln_repo_map[repo_name_db]:
                        vuln_repo_map[repo_name_db][vuln_id] = []

                    # If p.versions is a single string, add it directly, else extend the list
                    if isinstance(affected_versions, list):
                        for version in affected_versions:
                            if version not in vuln_repo_map[repo_name_db][vuln_id]:
                                vuln_repo_map[repo_name_db][vuln_id].append(version)
                    else:
                        if affected_versions not in vuln_repo_map[repo_name_db][vuln_id]:
                            vuln_repo_map[repo_name_db][vuln_id].append(affected_versions)

                batch_size_here = len(results)
                processed_count += batch_size_here

                if processed_count % progress_interval < self.batch_size:
                    elapsed = time.time() - start_time
                    percent = (processed_count / total_count) * 100 if total_count > 0 else 0
                    rps = processed_count / elapsed if elapsed > 0 else 0
                    eta_seconds = (total_count - processed_count) / rps if rps > 0 else 0

                    print(f"Progress: {processed_count}/{total_count} ({percent:.1f}%) - "
                          f"Speed: {rps:.1f} records/sec - "
                          f"ETA: {datetime.fromtimestamp(time.time() + eta_seconds).strftime('%H:%M:%S')}")

                skip += batch_size_here
                if batch_size_here < self.batch_size:
                    break

        print(f"Completed processing {processed_count} records in {time.time() - start_time:.1f} seconds")
        total_vulns_found = sum(len(vulns) for vulns in vuln_repo_map.values())
        print(f"Found {total_vulns_found} unique vulnerabilities")

        return vuln_repo_map

    def build_minimal_hitting_sets_for_repo(self, repo_name="OSV"):
        """
        Builds a minimal hitting set for all CVEs in the specified repository.
        
        This implements Task 3 of Part 1:
        1) Gets all CVEs for the repository with their affected versions
        2) Converts the data into list-of-lists format for the solver
        3) Solves the minimum hitting set problem
        4) Stores the minimal version set in Neo4j for later reference
        
        Args:
            repo_name: Repository name to build the hitting set for (default: "OSV")
            
        Returns:
            Dictionary with repository name, minimal cover list, and size
            e.g., {"OSV": {"minimal_cover": ["v1.1", "v1.2"], "min_cover_size": 2}}
        """
        import time
        import logging

        logger = logging.getLogger(__name__)
        logger.info(f"Starting build_minimal_hitting_sets_for_repo for {repo_name}")

        start_time = time.time()

        print("DEBUG: Entering build_minimal_hitting_sets_for_repo")
        print(f"DEBUG: Repository name: {repo_name}")

        # Step 1: Get vulnerability-version mapping for the repository
        logger.info("Fetching vulnerability-version mapping from neo4j")
        vuln_repo_map = self.get_vulnerability_repo_mapping_batched(repo_name=repo_name)

        fetch_time = time.time() - start_time
        logger.info(f"Fetched vulnerability mapping in {fetch_time:.2f} seconds")
        
        print(f"DEBUG: Total repositories in map: {len(vuln_repo_map)}")
        print(f"DEBUG: Repositories found: {list(vuln_repo_map.keys())}")

        # Handle case where repository doesn't exist
        if repo_name not in vuln_repo_map:
            logger.warning(f"No data found for repo {repo_name}")
            return {repo_name: {"minimal_cover": [], "min_cover_size": 0}}
        
        # Get the CVE-to-versions dictionary for this repository
        cve_dict = vuln_repo_map[repo_name]
        cve_count = len(cve_dict)
        logger.info(f"Found {cve_count} CVEs for {repo_name}")
        print(f"DEBUG: Total CVEs for {repo_name}: {len(cve_dict)}")

        if not cve_dict:
            logger.warning(f"No CVEs found for repo {repo_name}")
            print(f"No CVEs found for repo {repo_name}.")
            return {repo_name: {"minimal_cover": [], "min_cover_size": 0}}
        
        # Sample and log information about the data to help with debugging
        if cve_count > 0:
            # Get a sample of CVEs for logging
            sample_size = min(5, cve_count)
            sample_keys = list(cve_dict.keys())[:sample_size]
            
            for key in sample_keys:
                versions = cve_dict[key]
                logger.info(f"Sample CVE {key}: {len(versions)} versions, example: {versions[:3]}...")

        # Step 2: Convert the dictionary into a list-of-lists format for the solver
        cve_version_lists = list(cve_dict.values())
        print(f"Found {len(cve_version_lists)} CVEs with version data for {repo_name}")
        
        # Step 3: Generate recency scores for the versions
        logger.info(f"Generating semantic version recency scores for all versions")
        version_recency_start = time.time()
        version_recency = self._get_semantic_version_recency(cve_version_lists)
        version_recency_time = time.time() - version_recency_start

        version_count = len(version_recency)
        logger.info(f"Generated recency scores for {version_count} unique versions in {version_recency_time:.2f} seconds")
        
        # Sample a few recency scores for debugging
        if version_count > 0:
            sample_versions = list(version_recency.keys())[:5]
            for v in sample_versions:
                logger.info(f"Version {v} has recency score {version_recency[v]}")
    
        # Step 3b: Solve the Minimum Hitting Set problem
        logger.info(f"Solving minimum hitting set problem for {cve_count} CVEs with {version_count} unique versions")
        solver_start_time = time.time()

        # Step 4: Solve the Minimum Hitting Set problem
        print(f"Solving minimum hitting set problem for {len(cve_version_lists)} CVEs...")
        minimal_cover = find_minimum_hitting_set(cve_version_lists, version_recency)
        
        solver_time = time.time() - solver_start_time
        logger.info(f"Minimum hitting set solver completed in {solver_time:.2f} seconds")
        logger.info(f"Found minimal cover with {len(minimal_cover)} versions")

        # For debugging, log a subset of the minimal cover if it's large
        if minimal_cover:
            log_limit = min(10, len(minimal_cover))
            logger.info(f"First {log_limit} versions in minimal cover: {minimal_cover[:log_limit]}")
    
        # Step 4: Verify the solution
        if minimal_cover:
            # Count how many CVEs are covered by this solution
            covered_cves = 0
            for vuln_id, versions in cve_dict.items():
                if any(v in minimal_cover for v in versions):
                    covered_cves += 1
        
            coverage_pct = (covered_cves / cve_count) * 100
            logger.info(f"Solution verification: covers {covered_cves}/{cve_count} CVEs ({coverage_pct:.1f}%)")
        
            # If we didn't cover 100%, something is wrong with our algorithm
            if covered_cves < cve_count:
                logger.error("ERROR: Solution does not cover all CVEs - algorithm error detected!")
    
        # Step 5: Store the minimal cover in Neo4j
        logger.info(f"Storing minimal cover in Neo4j for repo {repo_name}")
        store_start_time = time.time()
        self._store_minimal_cover_in_neo4j(repo_name, minimal_cover)
        store_time = time.time() - store_start_time
        logger.info(f"Storing minimal cover in Neo4j took {store_time:.2f} seconds")
    
        # Return the result
        total_time = time.time() - start_time
        logger.info(f"Total processing time for build_minimal_hitting_sets_for_repo: {total_time:.2f} seconds")
    
        return {
            repo_name: {
                "minimal_cover": minimal_cover,
                "min_cover_size": len(minimal_cover)
            }
        }
    
    def build_minimal_hitting_sets_per_package(self, repo_name="OSV", filename=None, batch_size=100):
        """
        Build minimal hitting sets for each package in the repository, processing in batches.
        
        Args:
            repo_name: Repository name (default: "OSV")
            filename: Output file name for the JSON (optional)
            batch_size: Number of packages to process in each batch
            
        Returns:
            Dictionary containing package-organized data with minimal hitting sets
        """
        import time
        import logging
        
        logger = logging.getLogger(__name__)
        logger.info(f"Starting build_minimal_hitting_sets_per_package for {repo_name}")
        
        start_time = time.time()
        
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f'package_minimal_sets_{repo_name}_{timestamp}.json'
        
        # Initialize results structure
        results = {}
        
        # Query data organized by package
        if not self._driver:
            print("Error: Not connected to Neo4j. Call connect() first.")
            return None
        
        try:
            with self._driver.session() as session:
                # Step 1: Get all unique package names first
                package_query = """
                    MATCH (v:Vulnerability)-[:BELONGS_TO]->(vr:VULN_REPO)
                    MATCH (v)-[:AFFECTS]->(p:Package)
                    WHERE vr.name = $repo_name
                    RETURN DISTINCT p.name AS package_name
                """
                
                all_packages = []
                package_count = 0
                
                # Use a short transaction to get package names
                tx_config = {"timeout": 60}  # 60 seconds timeout
                try:
                    with session.begin_transaction(**tx_config) as tx:
                        for record in tx.run(package_query, {"repo_name": repo_name}):
                            package_name = record["package_name"]
                            if package_name:  # Skip empty names
                                all_packages.append(package_name)
                                package_count += 1
                    
                    print(f"Found {package_count} packages to process")
                except Exception as e:
                    print(f"Error getting package names: {e}")
                    # Continue with an empty list
                
                # If no packages found through transaction, try a different approach
                if not all_packages:
                    print("Trying alternative approach to get package names...")
                    try:
                        # Use a direct query without transaction
                        result = session.run(package_query, {"repo_name": repo_name})
                        for record in result:
                            package_name = record["package_name"]
                            if package_name:
                                all_packages.append(package_name)
                        
                        package_count = len(all_packages)
                        print(f"Found {package_count} packages with alternative approach")
                    except Exception as e:
                        print(f"Alternative approach also failed: {e}")
                
                # Step 2: Process packages in batches
                processed = 0
                successful = 0
                
                for i in range(0, len(all_packages), batch_size):
                    batch = all_packages[i:i+batch_size]
                    batch_num = i // batch_size + 1
                    total_batches = (len(all_packages) - 1) // batch_size + 1
                    
                    print(f"Processing batch {batch_num}/{total_batches} ({len(batch)} packages)")
                    
                    for package_name in batch:
                        try:
                            # Get vulnerabilities for this package
                            package_query = """
                                MATCH (v:Vulnerability)-[:BELONGS_TO]->(vr:VULN_REPO)
                                MATCH (v)-[:AFFECTS]->(p:Package)
                                WHERE vr.name = $repo_name AND p.name = $package_name
                                RETURN p.name AS package_name, p.ecosystem AS ecosystem, 
                                    v.id AS vuln_id, p.versions AS affected_versions
                            """
                            
                            # Process a single package in its own transaction
                            package_tx_config = {"timeout": 30}  # 30 seconds per package
                            vulns = {}
                            ecosystem = None
                            
                            with session.begin_transaction(**package_tx_config) as tx:
                                for record in tx.run(package_query, {"repo_name": repo_name, "package_name": package_name}):
                                    ecosystem = record['ecosystem']
                                    vuln_id = record['vuln_id']
                                    affected_versions = record['affected_versions']
                                    
                                    # Process versions
                                    versions_list = []
                                    if isinstance(affected_versions, list):
                                        versions_list.extend([v for v in affected_versions if v])
                                    elif affected_versions:
                                        versions_list.append(affected_versions)
                                    
                                    # Add to vulnerabilities dict
                                    if vuln_id and versions_list:
                                        vulns[vuln_id] = versions_list
                            
                            # Skip if no valid vulnerabilities
                            if not vulns:
                                print(f"  - Package {package_name}: No vulnerabilities found, skipping")
                                continue
                            
                            # Extract version lists for this package
                            version_lists = list(vulns.values())
                            
                            # Generate recency scores for this package
                            version_recency = self._get_semantic_version_recency(version_lists)
                            
                            # Find minimum hitting set for this package
                            start_solve = time.time()
                            minimal_cover = find_minimum_hitting_set(version_lists, version_recency)
                            solve_time = time.time() - start_solve
                            
                            # Store in results
                            results[package_name] = {
                                "ecosystem": ecosystem,
                                "vulnerabilities": vulns,
                                "minimal_versions": minimal_cover,
                                "min_cover_size": len(minimal_cover)
                            }
                            
                            successful += 1
                            
                            print(f"  - Package {package_name}: {len(vulns)} vulnerabilities, {len(minimal_cover)} versions in minimal set ({solve_time:.1f}s)")
                        
                        except Exception as e:
                            print(f"  - Error processing package {package_name}: {e}")
                        
                        processed += 1
                    
                    # Save intermediate results after each batch
                    try:
                        with open(filename, 'w') as f:
                            json.dump(results, f, indent=2)
                        
                        print(f"Saved intermediate results ({len(results)} packages) to {filename}")
                    except Exception as e:
                        print(f"Error writing intermediate results: {e}")
                    
                    # Progress report
                    elapsed = time.time() - start_time
                    print(f"Progress: {processed}/{package_count} packages ({processed/package_count*100:.1f}%) - "
      f"Success rate: {successful}/{processed} ({(successful/processed*100) if processed > 0 else 0:.1f}%) - "
      f"Elapsed: {elapsed:.1f}s")
                
                # Write final results to file
                try:
                    with open(filename, 'w') as f:
                        json.dump(results, f, indent=2)
                    print(f"Exported package-based minimal hitting sets to {filename}")
                except Exception as e:
                    print(f"Error writing to file: {e}")
                
                total_time = time.time() - start_time
                print(f"Completed in {total_time:.1f} seconds")
                return results
            
        except Exception as e:
            print(f"Error building minimal hitting sets per package: {e}")
            import traceback
            traceback.print_exc()
            return results if results else None
            

    def _store_minimal_cover_in_neo4j(self, repo_name, cover_list):
        """
        Store the minimal version cover in Neo4j for a given repository.
        
        Args:
            repo_name: Name of the repository (e.g., "OSV")
            cover_list: List of versions making up the minimal cover
            
        Returns:
            None
        """
        if not self._driver:
            print("Error: Not connected to Neo4j. Call connect() first.")
            return
        
        if not cover_list:
            print(f"Warning: Empty minimal cover for {repo_name}, skipping Neo4j update.")
            return
        
        print(f"Storing minimal cover with {len(cover_list)} versions for repo {repo_name} in Neo4j...")
        
        try:
            with self._driver.session() as session:
                # Update or create the VULN_REPO node with the minimal_versions property
                query = """
                MERGE (r:VULN_REPO {name: $repo_name})
                SET r.minimal_versions = $versions,
                    r.minimal_versions_count = size($versions),
                    r.minimal_versions_updated = datetime()
                RETURN r.name as repo_name, r.minimal_versions_count as count
                """
                
                result = session.run(
                    query, 
                    repo_name=repo_name, 
                    versions=cover_list
                )
                
                # Get the first record from the result to confirm success
                record = result.single()
                if record:
                    print(f"Successfully updated {record['repo_name']} with {record['count']} minimal versions in Neo4j.")
                else:
                    print(f"Warning: Query executed but no confirmation returned for {repo_name}.")
        
        except Exception as e:
            print(f"Error storing minimal cover in Neo4j: {e}")


    def _get_semantic_version_recency(self, cve_version_lists):
        """
        Create a recency score mapping for all versions that properly handles
        semantic versioning patterns (e.g., v1.2.3, 1.2.3-alpha, etc.).
        
        Args:
            cve_version_lists: List of lists containing affected versions for each CVE
            
        Returns:
            Dictionary mapping each version to a recency score (higher = more recent)
        """
        import re
        from datetime import datetime
        
        # Collect all unique versions
        all_versions = set()
        for sublist in cve_version_lists:
            all_versions.update(sublist)
        
        # Define regex patterns for version strings
        # Pattern for semantic versions like v1.2.3 or 1.2.3-alpha
        semver_pattern = re.compile(r'^v?(\d+)(?:[.-](\d+))?(?:[.-](\d+))?(?:[.-](\d+))?(?:[.-]([a-zA-Z0-9-]+))?$')
        # Pattern for date-based versions like 2021-03-15 or 20210315
        date_pattern = re.compile(r'^(\d{4})-?(\d{2})-?(\d{2})$')
        
        # Store parsed version components
        version_components = {}
        
        # Process each version string
        for version in all_versions:
            # Try semver pattern first
            semver_match = semver_pattern.match(version)
            if semver_match:
                # Extract numeric components, fill with zeros if missing
                components = []
                for i in range(1, 5):  # Groups 1-4 are version numbers
                    group = semver_match.group(i)
                    components.append(int(group) if group and group.isdigit() else 0)
                
                # Check for suffix (alpha, beta, etc.) that should reduce priority
                suffix = semver_match.group(5)
                suffix_penalty = 0
                if suffix:
                    # Pre-releases should be ranked lower than final releases
                    suffix_lower = suffix.lower()
                    if 'alpha' in suffix_lower:
                        suffix_penalty = -30
                    elif 'beta' in suffix_lower:
                        suffix_penalty = -20
                    elif 'rc' in suffix_lower or 'pre' in suffix_lower:
                        suffix_penalty = -10
                
                # Add suffix penalty to last component
                components[3] += suffix_penalty
                
                version_components[version] = components
                continue
            
            # Try date pattern
            date_match = date_pattern.match(version)
            if date_match:
                try:
                    year, month, day = map(int, date_match.groups())
                    # Convert date to numeric value (days since epoch)
                    date_obj = datetime(year, month, day)
                    epoch = datetime(1970, 1, 1)
                    days = (date_obj - epoch).days
                    # Use high first component to rank dates above regular versions
                    version_components[version] = [10000, days, 0, 0]
                    continue
                except (ValueError, OverflowError):
                    # Invalid date, fall through to default handling
                    pass
            
            # Default handling for non-matched versions
            # Convert to string and use character codes as components
            version_components[version] = [0, 0, 0, 0]  # Lowest priority
        
        # Sort versions based on components
        sorted_versions = sorted(version_components.items(), key=lambda x: tuple(x[1]))
        
        # Create recency scores (higher = more recent)
        version_recency = {}
        base_score = 100
        for i, (version, _) in enumerate(sorted_versions):
            version_recency[version] = base_score + i
        
        return version_recency

    def export_to_json(self, repo_name="OSV", filename=None):
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f'vulnerability_repo_map_{repo_name}_{timestamp}.json'

        print(f"Starting export to {filename}...")
        start_time = time.time()

        vuln_repo_map = self.get_vulnerability_repo_mapping_batched(repo_name)
        if not vuln_repo_map:
            print("No data to export.")
            return False

        with open(filename, 'w') as f:
            json.dump(vuln_repo_map, f, indent=2)

        print(f"Export completed in {time.time() - start_time:.1f} seconds")
        print(f"Exported vulnerability repo mapping to {filename}")
        return True

    def export_to_json_streaming(self, filename=None, progress_interval=10000):
        if not self._driver:
            print("Error: Not connected to Neo4j. Call connect() first.")
            return False

        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f'package_cve_versions_{timestamp}.json'

        print(f"Starting streaming export to {filename}...")
        start_time = time.time()

        total_count = self.get_vulnerability_count()
        print(f"Processing {total_count} vulnerability relationships...")

        processed_count = 0
        package_count = 0

        with open(filename, 'w') as f:
            f.write("{\n")  # Start the JSON object

            is_first_package = True
            with self._driver.session() as session:
                query = """
                    MATCH (v:Vulnerability)-[:AFFECTS]->(p:Package)
                    RETURN p.name AS package_name, p.ecosystem AS ecosystem, v.id AS vuln_id,
                           collect(p.versions) AS affected_versions
                    ORDER BY p.name, v.id
                """
                result = session.run(query)

                current_package = None
                is_first_vuln = True

                for record in result:
                    package_name = record['package_name']
                    ecosystem = record['ecosystem']
                    vuln_id = record['vuln_id']
                    all_versions = record['affected_versions']

                    affected_versions = []
                    for version_item in all_versions:
                        if isinstance(version_item, list):
                            affected_versions.extend(version_item)
                        else:
                            affected_versions.append(version_item)

                    unique_versions = list(set(affected_versions))

                    if package_name != current_package:
                        if current_package is not None:
                            f.write("\n        },\n")

                        if not is_first_package:
                            f.write(",\n")

                        f.write(f'        "{package_name}": {{\n')
                        f.write(f'            "ecosystem": "{ecosystem}",\n')
                        current_package = package_name
                        is_first_vuln = True
                        package_count += 1
                        is_first_package = False

                    if not is_first_vuln:
                        f.write(",\n")

                    f.write(f'            "{vuln_id}": {json.dumps(unique_versions, indent=12)}')
                    is_first_vuln = False
                    processed_count += 1

                    if processed_count % progress_interval == 0:
                        elapsed = time.time() - start_time
                        percent = (processed_count / total_count) * 100 if total_count > 0 else 0
                        rps = processed_count / elapsed if elapsed > 0 else 0
                        eta_seconds = (total_count - processed_count) / rps if rps > 0 else 0
                        print(f"Progress: {processed_count}/{total_count} ({percent:.1f}%) - "
                              f"Packages: {package_count} - "
                              f"Speed: {rps:.1f} records/sec - "
                              f"ETA: {datetime.fromtimestamp(time.time() + eta_seconds).strftime('%H:%M:%S')}")

                if current_package is not None:
                    f.write("\n        }\n")

                f.write("}\n")

        print(f"Completed processing {processed_count} records in {time.time() - start_time:.1f} seconds")
        print(f"Found {package_count} unique packages")
        print(f"Exported package CVE versions to {filename}")
        return True
    
    def validate_package_coverage(self, results):
        """
        Validate that the minimal version set for each package covers all its CVEs.
        
        Args:
            results: Dictionary output from build_minimal_hitting_sets_per_package
            
        Returns:
            Dictionary with validation statistics
        """
        validation_stats = {
            "total_packages": len(results),
            "packages_with_full_coverage": 0,
            "packages_with_partial_coverage": 0,
            "coverage_percentages": {},
            "problematic_packages": []
        }
        
        for package_name, package_data in results.items():
            minimal_versions = package_data.get("minimal_versions", [])
            vulnerabilities = package_data.get("vulnerabilities", {})
            
            if not vulnerabilities or not minimal_versions:
                validation_stats["problematic_packages"].append({
                    "package": package_name,
                    "issue": "Missing data",
                    "vuln_count": len(vulnerabilities),
                    "min_set_size": len(minimal_versions)
                })
                continue
            
            # Check coverage for each CVE
            total_cves = len(vulnerabilities)
            covered_cves = 0
            
            for cve_id, affected_versions in vulnerabilities.items():
                # A CVE is covered if any of its affected versions is in the minimal set
                if any(version in minimal_versions for version in affected_versions):
                    covered_cves += 1
            
            # Calculate coverage percentage
            coverage_pct = (covered_cves / total_cves) * 100 if total_cves > 0 else 0
            validation_stats["coverage_percentages"][package_name] = coverage_pct
            
            if coverage_pct == 100:
                validation_stats["packages_with_full_coverage"] += 1
            else:
                validation_stats["packages_with_partial_coverage"] += 1
                validation_stats["problematic_packages"].append({
                    "package": package_name,
                    "issue": f"Partial coverage: {coverage_pct:.1f}%",
                    "covered": f"{covered_cves}/{total_cves}"
                })
        
        # Overall coverage statistics
        all_percentages = list(validation_stats["coverage_percentages"].values())
        validation_stats["average_coverage"] = sum(all_percentages) / len(all_percentages) if all_percentages else 0
        validation_stats["min_coverage"] = min(all_percentages) if all_percentages else 0
        validation_stats["max_coverage"] = max(all_percentages) if all_percentages else 0
        
        return validation_stats
    
    def generate_final_report(self, results, validation_stats, output_file="final_report.json"):
        """
        Generate a final report with summary and details for all packages.
        
        Args:
            results: Dictionary output from build_minimal_hitting_sets_per_package
            validation_stats: Dictionary output from validate_package_coverage
            output_file: Filename for the output JSON
        """
        report = {
            "summary": {
                "total_packages": validation_stats["total_packages"],
                "total_cves": sum(len(pkg["vulnerabilities"]) for pkg in results.values()),
                "full_coverage_percent": (validation_stats["packages_with_full_coverage"] / 
                                        validation_stats["total_packages"] * 100) if validation_stats["total_packages"] > 0 else 0,
                "average_coverage": validation_stats["average_coverage"],
                "timestamp": datetime.now().strftime("%Y-%m-%d %H:%M:%S"),
                "average_minimal_set_size": sum(pkg.get("min_cover_size", 0) for pkg in results.values()) / 
                                            len(results) if results else 0
            },
            "packages": {}
        }
        
        # Include detailed information for each package
        for package_name, package_data in results.items():
            vuln_count = len(package_data.get("vulnerabilities", {}))
            min_set = package_data.get("minimal_versions", [])
            coverage = validation_stats["coverage_percentages"].get(package_name, 0)
            
            report["packages"][package_name] = {
                "ecosystem": package_data.get("ecosystem", "unknown"),
                "vulnerability_count": vuln_count,
                "minimal_version_set": min_set,
                "minimal_set_size": len(min_set),
                "coverage_percentage": coverage
            }
        
        # Save the report
        with open(output_file, 'w') as f:
            json.dump(report, f, indent=2)
        
        print(f"Final report saved to {output_file}")
        
        # Also create a simplified CSV version for easy analysis
        csv_file = output_file.replace(".json", ".csv")
        with open(csv_file, 'w') as f:
            f.write("Package,Ecosystem,Vulnerability Count,Minimal Set Size,Coverage Percentage\n")
            for package, data in report["packages"].items():
                f.write(f"{package},{data['ecosystem']},{data['vulnerability_count']},"
                    f"{data['minimal_set_size']},{data['coverage_percentage']}\n")
        
        print(f"CSV summary saved to {csv_file}")
        
        return report

def find_minimum_hitting_set(cve_version_lists, version_recency=None):
    """
    Returns a minimal set of versions that covers all CVEs in cve_version_lists.
    Breaks ties by choosing the set with maximum sum of recency scores.
    
    Args:
        cve_version_lists: List of lists where each sublist contains versions affected by a specific CVE
        version_recency: Dictionary mapping versions to their recency scores (higher = more recent)
                         If None, will not prioritize by recency
    
    Returns:
        List of versions forming the minimum hitting set
    """
    from ortools.sat.python import cp_model
    import time
    import logging

    # sets up logging

    logger = logging.getLogger(__name__)

    start_time = time.time()

    # input validation
    if not isinstance(cve_version_lists, list):
        logger.error(f"Input must be a list of lists, got {type(cve_version_lists)}")
        return []

    # Filter out empty version lists first
    non_empty_lists = [lst for lst in cve_version_lists if lst]
    if len(non_empty_lists) != len(cve_version_lists):
        filtered_out = len(cve_version_lists) - len(non_empty_lists)
        logger.warning(f"Filtered out {filtered_out} empty CVE version lists")

    # Edge-case handling: empty input or any empty set makes hitting impossible
    if not non_empty_lists:
        logger.warning("No valid CVE version lists to process")
        return []
    
    # Get all unique versions across all CVEs
    all_versions = sorted(set(v for sublist in non_empty_lists for v in sublist))
    if not all_versions:
        logger.warning("No versions found in the input lists")
        return []
    
    logger.info(f"Processing {len(non_empty_lists)} CVEs with {len(all_versions)} unique versions")

    # If version_recency is not provided, create a default one with all zeros
    if version_recency is None:
        version_recency = {}
    
    # Create a dictionary with recency scores, defaulting to 0 if not specified
    rec = {v: version_recency.get(v, 0) for v in all_versions}
    
    logger.info("Phase 1: Finding minimum hitting set cardinality")
    phase1_start = time.time()

    try: 
        # Set up the CP-SAT model
        model = cp_model.CpModel()
        
        # Create boolean variables for each version
        x = {}
        for v in all_versions:
            x[v] = model.NewBoolVar(v)  # Boolean variable indicating if version v is in the solution

        # Add constraints to ensure each CVE is covered by at least one version
        for i, cve_list in enumerate(non_empty_lists):
            model.Add(sum(x[v] for v in cve_list) >= 1)
    
        # Create a variable for the total number of versions in the solution
        cardinality = model.NewIntVar(0, len(all_versions), "cardinality")
        model.Add(cardinality == sum(x[v] for v in all_versions))

        # Minimize the number of versions
        model.Minimize(cardinality)
    
        # set up solver with timeout to avoid hanging on very large problems
        solver = cp_model.CpSolver()
        solver.parameters.max_time_in_seconds = 300  # 5 min timeout

        # Solve phase 1
        status = solver.Solve(model)

        # Handle solver status
        if status == cp_model.OPTIMAL:
            logger.info("Found optimal solution in phase 1")
        elif status == cp_model.FEASIBLE:
            logger.info("Found feasible (but possibly not optimal) solution in phase 1")
        else:
            logger.warning(f"Phase 1 solver status: {status} - no solution found")
            return []
        
        # Get the minimum cardinality value
        min_cardinality = int(solver.ObjectiveValue())
        logger.info(f"Minimum hitting set cardinality: {min_cardinality}")

         # If all recency scores are the same (or zero), we can skip phase 2
        if len(set(rec.values())) <= 1:
            logger.info("All versions have same recency score - skipping phase 2")
            chosen_versions = []
            for v in all_versions:
                if solver.BooleanValue(x[v]):
                    chosen_versions.append(v)

            phase1_time = time.time() - phase1_start
            logger.info(f"Phase 1 completed in {phase1_time:.2f} seconds")
            return chosen_versions
        
        # Phase 2: fix cardinality to min value and maximize recency
        logger.info("Phase 2: Maximizing recency while maintaining minimum cardinality")
        phase2_start = time.time()

        model2 = cp_model.CpModel()
        x2 = {}
        for v in all_versions:
            x2[v] = model2.NewBoolVar(v)

        # Copy constraints from phase 1
        for i, cve_list in enumerate(non_empty_lists):
            model2.Add(sum(x2[v] for v in cve_list) >= 1)

         # Fix cardinality to the minimum found in phase 1
        cardinality2 = model2.NewIntVar(0, len(all_versions), "cardinality2")
        model2.Add(cardinality2 == sum(x2[v] for v in all_versions))
        model2.Add(cardinality2 == min_cardinality)
        
        # We need to scale recency scores to avoid floating-point issues in the solver
        # Find the max recency value to help with scaling
        max_recency = max(rec.values()) if rec else 1


         # Multiply by 1000 and convert to int to handle decimal values
        # This creates a scaled version of the objective function
        scale_factor = 1000
        scaled_terms = []
        for v in all_versions:
            # Scale and convert to int
            scaled_value = int(rec[v] * scale_factor)
            # Only add non-zero terms
            if scaled_value != 0:
                scaled_terms.append(x2[v] * scaled_value)

        # Create objective variable with appropriate bounds
        max_possible_sum = len(all_versions) * max_recency * scale_factor
        recency_sum = model2.NewIntVar(0, max_possible_sum, "recency_sum")

        # If we have scaled terms, use them; otherwise use a default sum of zeros
        if scaled_terms:
            model2.Add(recency_sum == sum(scaled_terms))
        else:
            model2.Add(recency_sum == 0)

        # Set objective to maximize recency
        model2.Maximize(recency_sum)
        
        # Solve phase 2
        solver2 = cp_model.CpSolver()
        solver2.parameters.max_time_in_seconds = 300  # 5-minute timeout
        status2 = solver2.Solve(model2)
        
        # Handle solver status
        if status2 == cp_model.OPTIMAL:
            logger.info("Found optimal solution in phase 2")
        elif status2 == cp_model.FEASIBLE:
            logger.info("Found feasible (but possibly not optimal) solution in phase 2")
        else:
            logger.warning(f"Phase 2 solver status: {status2} - falling back to phase 1 solution")
            # Fall back to phase 1 solution
            chosen_versions = []
            for v in all_versions:
                if solver.BooleanValue(x[v]):
                    chosen_versions.append(v)
            
            total_time = time.time() - start_time
            logger.info(f"Total solving time: {total_time:.2f} seconds")
            return chosen_versions
        
        # Build solution from phase 2
        chosen_versions = []
        for v in all_versions:
            if solver2.BooleanValue(x2[v]):
                chosen_versions.append(v)
        
        phase2_time = time.time() - phase2_start
        total_time = time.time() - start_time
        logger.info(f"Phase 2 completed in {phase2_time:.2f} seconds")
        logger.info(f"Total solving time: {total_time:.2f} seconds")
        
        # Verify the solution is valid
        covered_cves = 0
        for cve_list in non_empty_lists:
            if any(v in chosen_versions for v in cve_list):
                covered_cves += 1
            else:
                logger.error(f"Solution verification failed: CVE list {cve_list} not covered by {chosen_versions}")
                
        coverage_pct = (covered_cves / len(non_empty_lists)) * 100
        logger.info(f"Solution covers {covered_cves}/{len(non_empty_lists)} CVEs ({coverage_pct:.1f}%)")
        
        return chosen_versions
    
    except Exception as e:
        logger.error(f"Error solving minimum hitting set: {e}")
        return []

def test_per_package_implementation():
    mapper = VulnerabilityRepoMapper(batch_size=10000)
    try:
        if mapper.connect():
            print("Connected to Neo4j database.")
            
            # Run with a small limit for testing first
            test_output_file = "test_package_minimal_sets.json"
            
            # Run the per-package implementation with a timestamp-based filename
            results = mapper.build_minimal_hitting_sets_per_package("OSV")
            
            # Verify the results
            if results:
                print(f"Successfully generated minimal hitting sets for {len(results)} packages")
                # Print a sample of the results
                sample_packages = list(results.keys())[:3]
                for package in sample_packages:
                    pkg_data = results[package]
                    vuln_count = len(pkg_data["vulnerabilities"])
                    min_set_size = pkg_data["min_cover_size"]
                    print(f"Package: {package}, Vulnerabilities: {vuln_count}, Minimal Set Size: {min_set_size}")
            else:
                print("No results returned from per-package implementation")
    finally:
        mapper.close()

def run_and_validate_per_package():
    mapper = VulnerabilityRepoMapper(batch_size=10000)
    try:
        if mapper.connect():
            print("Connected to Neo4j database.")
            
            # Run the per-package implementation
            results = mapper.build_minimal_hitting_sets_per_package("OSV")
            
            if results:
                # Validate the results
                print("Validating package coverage...")
                validation_stats = mapper.validate_package_coverage(results)
                
                # Print validation summary
                print("\n--- Validation Summary ---")
                print(f"Total packages: {validation_stats['total_packages']}")
                print(f"Packages with 100% coverage: {validation_stats['packages_with_full_coverage']} "
                      f"({validation_stats['packages_with_full_coverage']/validation_stats['total_packages']*100:.1f}%)")
                print(f"Packages with partial coverage: {validation_stats['packages_with_partial_coverage']}")
                print(f"Average coverage: {validation_stats['average_coverage']:.1f}%")
                print(f"Min coverage: {validation_stats['min_coverage']:.1f}%")
                print(f"Max coverage: {validation_stats['max_coverage']:.1f}%")
                
                # Show problematic packages
                if validation_stats["problematic_packages"]:
                    print("\n--- Problematic Packages ---")
                    for i, pkg in enumerate(validation_stats["problematic_packages"][:10]):  # Show first 10
                        print(f"{i+1}. {pkg['package']}: {pkg['issue']}")
                    
                    if len(validation_stats["problematic_packages"]) > 10:
                        print(f"...and {len(validation_stats['problematic_packages']) - 10} more.")
                
                # Save validation report
                validation_file = "package_coverage_validation.json"
                with open(validation_file, 'w') as f:
                    json.dump(validation_stats, f, indent=2)
                print(f"\nDetailed validation results saved to {validation_file}")
    finally:
        mapper.close()

def run_complete_pipeline():
    mapper = VulnerabilityRepoMapper(batch_size=10000)
    try:
        if mapper.connect():
            print("Connected to Neo4j database.")
            
            # Step 1: Run the per-package implementation
            print("\n=== STEP 1: Computing Minimal Hitting Sets Per Package ===")
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            output_file = f"package_minimal_sets_{timestamp}.json"
            
            results = mapper.build_minimal_hitting_sets_per_package("OSV", filename=output_file)
            
            if results:
                # Step 2: Validate the results
                print("\n=== STEP 2: Validating Coverage ===")
                validation_stats = mapper.validate_package_coverage(results)
                
                # Print validation summary
                print("\n--- Validation Summary ---")
                print(f"Total packages: {validation_stats['total_packages']}")
                print(f"Packages with 100% coverage: {validation_stats['packages_with_full_coverage']} "
                      f"({validation_stats['packages_with_full_coverage']/validation_stats['total_packages']*100:.1f}%)")
                
                # Step 3: Generate final report
                print("\n=== STEP 3: Generating Final Report ===")
                final_report = mapper.generate_final_report(results, validation_stats, 
                                                    f"final_report_{timestamp}.json")
                
                print("\nComplete pipeline executed successfully.")
                return final_report
            else:
                print("No results generated from per-package implementation.")
                return None
    finally:
        mapper.close()

if __name__ == "__main__":
    print("Running vulnerability mapper module directly.")
    # You can add code to run one of the functions above when the module is executed directly
    # For example: run_complete_pipeline()

def main():
    mapper = VulnerabilityRepoMapper(batch_size=10000)
    try:
        if mapper.connect():
            print("Successfully connected to Neo4j database.")

            # Optional: Clean up duplicates (if desired)
            # mapper.deduplicate_osv_nodes()

            total_vulns = mapper.get_vulnerability_count()
            print(f"Found {total_vulns} vulnerabilities for repo 'OSV'.")

            # Build minimal hitting sets per package
            print("Finding minimal hitting sets per package...")
            result = mapper.build_minimal_hitting_sets_per_package("OSV")
            print("Completed per-package minimal hitting sets")

            print(f"Results saved to file with data for {len(result) if result else 0} packages")

        else:
            print("Failed to connect to Neo4j database.")

    except Exception as e:
        print(f"An error occurred: {e}")
    finally:
        mapper.close()
