from neo4j import GraphDatabase
import json
import time
from datetime import datetime

from osv.neo4j_connection import get_neo4j_driver
import math
from ortools.sat.python import cp_model

class VulnerabilityRepoMapper:
    def __init__(self, batch_size=5000):
        self._driver = None
        self.batch_size = batch_size  # Number of records to process in each batch

    def connect(self):
        self._driver = get_neo4j_driver()
        return self._driver is not None

    def close(self):
        """Close the Neo4j connection if open"""
        if self._driver:
            self._driver.close()
            print("Neo4j connection closed.")

    def get_vulnerability_count(self):
        """Get the total count of vulnerabilities in the database for a specific repo"""
        if not self._driver:
            print("Error: Not connected to Neo4j. Call connect() first.")
            return 0

        with self._driver.session() as session:
            query = """
            MATCH (v:Vulnerability)-[:BELONGS_TO]->(vr:VULN_REPO)
            MATCH (v)-[:AFFECTS]->(p:Package)
            WHERE vr.name = 'OSV'
            RETURN COUNT(*) AS count
            """
            result = session.run(query)
            record = result.single()
            return record["count"] if record else 0

    def get_vulnerability_repo_mapping_batched(self, repo_name="OSV", progress_interval=10000):
        """
        Create a nested dictionary mapping vuln repositories to their vulnerabilities and affected versions.

        Output structure:
        {
          "OSV": {
            "CVE-1234": ["v1.0", "v1.2"],
            "CVE-5678": ["v1.1", "v1.3"],
            ...
          }
        }
        """
        if not self._driver:
            print("Error: Not connected to Neo4j. Call connect() first.")
            return {}

        total_count = self.get_vulnerability_count()
        print(f"Processing {total_count} vulnerabilities for repo '{repo_name}'...")

        vuln_repo_map = {}
        processed_count = 0
        start_time = time.time()

        with self._driver.session() as session:
            skip = 0

            while True:
                query = f"""
                MATCH (v:Vulnerability)-[:BELONGS_TO]->(vr:VULN_REPO)
                MATCH (v)-[:AFFECTS]->(p:Package)
                WHERE vr.name = $repo_name
                RETURN vr.name AS repo_name, v.id AS vuln_id, p.versions AS affected_versions
                SKIP {skip} LIMIT {self.batch_size}
                """

                results = list(session.run(query, {"repo_name": repo_name}))
                if not results:
                    break

                for record in results:
                    repo_name_db = record['repo_name']
                    vuln_id = record['vuln_id']
                    affected_versions = record['affected_versions']

                    if repo_name_db not in vuln_repo_map:
                        vuln_repo_map[repo_name_db] = {}

                    if vuln_id not in vuln_repo_map[repo_name_db]:
                        vuln_repo_map[repo_name_db][vuln_id] = []

                    # If p.versions is a single string, add it directly, else extend the list
                    if isinstance(affected_versions, list):
                        for version in affected_versions:
                            if version not in vuln_repo_map[repo_name_db][vuln_id]:
                                vuln_repo_map[repo_name_db][vuln_id].append(version)
                    else:
                        if affected_versions not in vuln_repo_map[repo_name_db][vuln_id]:
                            vuln_repo_map[repo_name_db][vuln_id].append(affected_versions)

                batch_size_here = len(results)
                processed_count += batch_size_here

                if processed_count % progress_interval < self.batch_size:
                    elapsed = time.time() - start_time
                    percent = (processed_count / total_count) * 100 if total_count > 0 else 0
                    rps = processed_count / elapsed if elapsed > 0 else 0
                    eta_seconds = (total_count - processed_count) / rps if rps > 0 else 0

                    print(f"Progress: {processed_count}/{total_count} ({percent:.1f}%) - "
                          f"Speed: {rps:.1f} records/sec - "
                          f"ETA: {datetime.fromtimestamp(time.time() + eta_seconds).strftime('%H:%M:%S')}")

                skip += batch_size_here
                if batch_size_here < self.batch_size:
                    break

        print(f"Completed processing {processed_count} records in {time.time() - start_time:.1f} seconds")
        total_vulns_found = sum(len(vulns) for vulns in vuln_repo_map.values())
        print(f"Found {total_vulns_found} unique vulnerabilities")

        return vuln_repo_map

    def build_minimal_hitting_sets_for_repo(self, repo_name="OSV"):
        """
        Builds a minimal hitting set for all CVEs in the specified repository.
        
        This implements Task 3 of Part 1:
        1) Gets all CVEs for the repository with their affected versions
        2) Converts the data into list-of-lists format for the solver
        3) Solves the minimum hitting set problem
        4) Stores the minimal version set in Neo4j for later reference
        
        Args:
            repo_name: Repository name to build the hitting set for (default: "OSV")
            
        Returns:
            Dictionary with repository name, minimal cover list, and size
            e.g., {"OSV": {"minimal_cover": ["v1.1", "v1.2"], "min_cover_size": 2}}
        """
        # Step 1: Get vulnerability-version mapping for the repository
        vuln_repo_map = self.get_vulnerability_repo_mapping_batched(repo_name=repo_name)
        
        # Handle case where repository doesn't exist
        if repo_name not in vuln_repo_map:
            print(f"No data found for repo {repo_name}.")
            return {repo_name: {"minimal_cover": [], "min_cover_size": 0}}
        
        # Get the CVE-to-versions dictionary for this repository
        cve_dict = vuln_repo_map[repo_name]
        if not cve_dict:
            print(f"No CVEs found for repo {repo_name}.")
            return {repo_name: {"minimal_cover": [], "min_cover_size": 0}}
        
        # Step 2: Convert the dictionary into a list-of-lists format for the solver
        cve_version_lists = list(cve_dict.values())
        print(f"Found {len(cve_version_lists)} CVEs with version data for {repo_name}")
        
        # Step 3: Generate recency scores for the versions
        version_recency = self._get_version_recency_stub(cve_version_lists)
        
        # Step 4: Solve the Minimum Hitting Set problem
        print(f"Solving minimum hitting set problem for {len(cve_version_lists)} CVEs...")
        minimal_cover = find_minimum_hitting_set(cve_version_lists, version_recency)
        
        print(f"Minimum hitting set solution contains {len(minimal_cover)} versions")
        if len(minimal_cover) <= 10:  # Only print full list if it's small
            print(f"Minimal cover: {minimal_cover}")
        else:
            print(f"Minimal cover first 10 elements: {minimal_cover[:10]}...")
        
        # Step 5: Store the minimal cover in Neo4j
        self._store_minimal_cover_in_neo4j(repo_name, minimal_cover)
        
        return {
            repo_name: {
                "minimal_cover": minimal_cover,
                "min_cover_size": len(minimal_cover)
            }
        }

    def _store_minimal_cover_in_neo4j(self, repo_name, cover_list):
        """
        Store the minimal version cover in Neo4j for a given repository.
        
        Args:
            repo_name: Name of the repository (e.g., "OSV")
            cover_list: List of versions making up the minimal cover
            
        Returns:
            None
        """
        if not self._driver:
            print("Error: Not connected to Neo4j. Call connect() first.")
            return
        
        if not cover_list:
            print(f"Warning: Empty minimal cover for {repo_name}, skipping Neo4j update.")
            return
        
        print(f"Storing minimal cover with {len(cover_list)} versions for repo {repo_name} in Neo4j...")
        
        try:
            with self._driver.session() as session:
                # Update or create the VULN_REPO node with the minimal_versions property
                query = """
                MERGE (r:VULN_REPO {name: $repo_name})
                SET r.minimal_versions = $versions,
                    r.minimal_versions_count = size($versions),
                    r.minimal_versions_updated = datetime()
                RETURN r.name, r.minimal_versions_count
                """
                
                result = session.run(
                    query, 
                    repo_name=repo_name, 
                    versions=cover_list
                )
                
                # Get the first record from the result to confirm success
                record = result.single()
                if record:
                    print(f"Successfully updated {record['name']} with {record['minimal_versions_count']} minimal versions in Neo4j.")
                else:
                    print(f"Warning: Query executed but no confirmation returned for {repo_name}.")
        
        except Exception as e:
            print(f"Error storing minimal cover in Neo4j: {e}")


    def _get_version_recency_stub(self, cve_version_lists):
        """
        Create a recency score mapping for all versions found in the CVE lists.
        In this stub implementation, we sort versions lexicographically and
        assign higher scores to later versions (assuming they are more recent).
        
        Args:
            cve_version_lists: List of lists containing affected versions for each CVE
            
        Returns:
            Dictionary mapping each version to a recency score (higher = more recent)
        """
        # Collect all unique versions
        all_versions = set()
        for sublist in cve_version_lists:
            all_versions.update(sublist)
        
        # Sort versions lexicographically (this is a simple heuristic)
        # In a real implementation, you would parse version strings and
        # sort according to semantic versioning rules
        version_list_sorted = sorted(all_versions)
        
        # Create mapping: higher index (later in sort) = higher recency score
        version_recency = {}
        score_start = 100  # Base score to start with
        for idx, ver in enumerate(version_list_sorted):
            version_recency[ver] = score_start + idx
        
        return version_recency

    def export_to_json(self, repo_name="OSV", filename=None):
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f'vulnerability_repo_map_{repo_name}_{timestamp}.json'

        print(f"Starting export to {filename}...")
        start_time = time.time()

        vuln_repo_map = self.get_vulnerability_repo_mapping_batched(repo_name)
        if not vuln_repo_map:
            print("No data to export.")
            return False

        with open(filename, 'w') as f:
            json.dump(vuln_repo_map, f, indent=2)

        print(f"Export completed in {time.time() - start_time:.1f} seconds")
        print(f"Exported vulnerability repo mapping to {filename}")
        return True

    def export_to_json_streaming(self, filename=None, progress_interval=10000):
        if not self._driver:
            print("Error: Not connected to Neo4j. Call connect() first.")
            return False

        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f'package_cve_versions_{timestamp}.json'

        print(f"Starting streaming export to {filename}...")
        start_time = time.time()

        total_count = self.get_vulnerability_count()
        print(f"Processing {total_count} vulnerability relationships...")

        processed_count = 0
        package_count = 0

        with open(filename, 'w') as f:
            f.write("{\n")  # Start the JSON object

            is_first_package = True
            with self._driver.session() as session:
                query = """
                    MATCH (v:Vulnerability)-[:AFFECTS]->(p:Package)
                    RETURN p.name AS package_name, p.ecosystem AS ecosystem, v.id AS vuln_id,
                           collect(p.versions) AS affected_versions
                    ORDER BY p.name, v.id
                """
                result = session.run(query)

                current_package = None
                is_first_vuln = True

                for record in result:
                    package_name = record['package_name']
                    ecosystem = record['ecosystem']
                    vuln_id = record['vuln_id']
                    all_versions = record['affected_versions']

                    affected_versions = []
                    for version_item in all_versions:
                        if isinstance(version_item, list):
                            affected_versions.extend(version_item)
                        else:
                            affected_versions.append(version_item)

                    unique_versions = list(set(affected_versions))

                    if package_name != current_package:
                        if current_package is not None:
                            f.write("\n        },\n")

                        if not is_first_package:
                            f.write(",\n")

                        f.write(f'        "{package_name}": {{\n')
                        f.write(f'            "ecosystem": "{ecosystem}",\n')
                        current_package = package_name
                        is_first_vuln = True
                        package_count += 1
                        is_first_package = False

                    if not is_first_vuln:
                        f.write(",\n")

                    f.write(f'            "{vuln_id}": {json.dumps(unique_versions, indent=12)}')
                    is_first_vuln = False
                    processed_count += 1

                    if processed_count % progress_interval == 0:
                        elapsed = time.time() - start_time
                        percent = (processed_count / total_count) * 100 if total_count > 0 else 0
                        rps = processed_count / elapsed if elapsed > 0 else 0
                        eta_seconds = (total_count - processed_count) / rps if rps > 0 else 0
                        print(f"Progress: {processed_count}/{total_count} ({percent:.1f}%) - "
                              f"Packages: {package_count} - "
                              f"Speed: {rps:.1f} records/sec - "
                              f"ETA: {datetime.fromtimestamp(time.time() + eta_seconds).strftime('%H:%M:%S')}")

                if current_package is not None:
                    f.write("\n        }\n")

                f.write("}\n")

        print(f"Completed processing {processed_count} records in {time.time() - start_time:.1f} seconds")
        print(f"Found {package_count} unique packages")
        print(f"Exported package CVE versions to {filename}")
        return True

def find_minimum_hitting_set(cve_version_lists, version_recency=None):
    """
    Returns a minimal set of versions that covers all CVEs in cve_version_lists.
    Breaks ties by choosing the set with maximum sum of recency scores.
    
    Args:
        cve_version_lists: List of lists where each sublist contains versions affected by a specific CVE
        version_recency: Dictionary mapping versions to their recency scores (higher = more recent)
                         If None, will not prioritize by recency
    
    Returns:
        List of versions forming the minimum hitting set
    """
    # Edge-case handling: empty input or any empty set makes hitting impossible
    if not cve_version_lists:
        return []
    
    for lst in cve_version_lists:
        if not lst:
            # If any CVE has no versions, it's impossible to create a hitting set
            return []

    # Get all unique versions across all CVEs
    all_versions = sorted(set(v for sublist in cve_version_lists for v in sublist))
    
    # If version_recency is not provided, create a default one with all zeros
    if version_recency is None:
        version_recency = {}
    
    # Create a dictionary with recency scores, defaulting to 0 if not specified
    rec = {v: version_recency.get(v, 0) for v in all_versions}
    
    # Set up the CP-SAT model
    from ortools.sat.python import cp_model
    model = cp_model.CpModel()
    
    # Create boolean variables for each version
    x = {}
    for v in all_versions:
        x[v] = model.NewBoolVar(v)  # Boolean variable indicating if version v is in the solution

    # Add constraints to ensure each CVE is covered by at least one version
    for cve_list in cve_version_lists:
        model.Add(sum(x[v] for v in cve_list) >= 1)
    
    # Create a variable for the total number of versions in the solution
    cardinality = model.NewIntVar(0, len(all_versions), "cardinality")
    model.Add(cardinality == sum(x[v] for v in all_versions))
    
    # Phase 1: Minimize the number of versions
    model.Minimize(cardinality)
    
    solver = cp_model.CpSolver()
    status = solver.Solve(model)
    
    # Handle solver failure
    if status not in (cp_model.OPTIMAL, cp_model.FEASIBLE):
        return []  # No solution found
    
    # Get the minimum cardinality value
    min_cardinality = int(solver.ObjectiveValue())
    
    # If all recency scores are zero, we can skip phase 2
    if all(r == 0 for r in rec.values()):
        chosen_versions = []
        for v in all_versions:
            if solver.BooleanValue(x[v]):
                chosen_versions.append(v)
        return chosen_versions
    
    # Phase 2: Fix cardinality to min value and maximize recency
    model2 = cp_model.CpModel()
    x2 = {}
    for v in all_versions:
        x2[v] = model2.NewBoolVar(v)
    
    # Copy constraints from phase 1
    for cve_list in cve_version_lists:
        model2.Add(sum(x2[v] for v in cve_list) >= 1)
    
    # Fix cardinality to the minimum found in phase 1
    cardinality2 = model2.NewIntVar(0, len(all_versions), "cardinality2")
    model2.Add(cardinality2 == sum(x2[v] for v in all_versions))
    model2.Add(cardinality2 == min_cardinality)
    
    # Create variable for recency sum and set objective to maximize it
    recency_sum = model2.NewIntVar(0, 10**12, "recency_sum")  # Using large upper bound
    model2.Add(recency_sum == sum(x2[v] * rec[v] for v in all_versions))
    model2.Maximize(recency_sum)
    
    # Solve phase 2
    solver2 = cp_model.CpSolver()
    status2 = solver2.Solve(model2)
    
    # If phase 2 fails, fall back to phase 1 solution
    if status2 not in (cp_model.OPTIMAL, cp_model.FEASIBLE):
        chosen_versions = []
        for v in all_versions:
            if solver.BooleanValue(x[v]):
                chosen_versions.append(v)
        return chosen_versions
    
    # Return the solution from phase 2
    chosen_versions = []
    for v in all_versions:
        if solver2.BooleanValue(x2[v]):
            chosen_versions.append(v)
    
    return chosen_versions

def main():
    # Example driver code:
    mapper = VulnerabilityRepoMapper(batch_size=10000)
    try:
        if mapper.connect():
            print("Successfully connected to Neo4j database.")

            # Optional: Clean up duplicates (if desired)
            # mapper.deduplicate_osv_nodes()

            total_vulns = mapper.get_vulnerability_count()
            print(f"Found {total_vulns} vulnerabilities for repo 'OSV'.")

            # Build minimal hitting set
            result = mapper.build_minimal_hitting_sets_for_repo("OSV")
            print("Minimum Hitting Set Results:")
            print(json.dumps(result, indent=2))

            # Example: also do an export
            if total_vulns > 100000:
                print("Large dataset -> streaming export...")
                mapper.export_to_json_streaming()
            else:
                print("Standard batch processing export...")
                mapper.export_to_json()

        else:
            print("Failed to connect to Neo4j database.")

    except Exception as e:
        print(f"An error occurred: {e}")
    finally:
        mapper.close()
