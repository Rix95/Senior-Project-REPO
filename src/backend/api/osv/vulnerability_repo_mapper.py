from neo4j import GraphDatabase
import json
import time
from datetime import datetime

from osv.neo4j_connection import get_neo4j_driver
import math
from ortools.sat.python import cp_model

class VulnerabilityRepoMapper:
    def __init__(self, batch_size=5000):
        self._driver = None
        self.batch_size = batch_size  # Number of records to process in each batch

    def connect(self):
        self._driver = get_neo4j_driver()
        return self._driver is not None

    def close(self):
        """Close the Neo4j connection if open"""
        if self._driver:
            self._driver.close()
            print("Neo4j connection closed.")

    def get_vulnerability_count(self):
        """Get the total count of vulnerabilities in the database for a specific repo"""
        if not self._driver:
            print("Error: Not connected to Neo4j. Call connect() first.")
            return 0

        with self._driver.session() as session:
            query = """
            MATCH (v:Vulnerability)-[:BELONGS_TO]->(vr:VULN_REPO)
            MATCH (v)-[:AFFECTS]->(p:Package)
            WHERE vr.name = 'OSV'
            RETURN COUNT(*) AS count
            """
            result = session.run(query)
            record = result.single()
            return record["count"] if record else 0

    def get_vulnerability_repo_mapping_batched(self, repo_name="OSV", progress_interval=10000):
        """
        Create a nested dictionary mapping vuln repositories to their vulnerabilities and affected versions.

        Output structure:
        {
          "OSV": {
            "CVE-1234": ["v1.0", "v1.2"],
            "CVE-5678": ["v1.1", "v1.3"],
            ...
          }
        }
        """
        if not self._driver:
            print("Error: Not connected to Neo4j. Call connect() first.")
            return {}

        total_count = self.get_vulnerability_count()
        print(f"Processing {total_count} vulnerabilities for repo '{repo_name}'...")

        vuln_repo_map = {}
        processed_count = 0
        start_time = time.time()

        with self._driver.session() as session:
            skip = 0

            while True:
                query = f"""
                MATCH (v:Vulnerability)-[:BELONGS_TO]->(vr:VULN_REPO)
                MATCH (v)-[:AFFECTS]->(p:Package)
                WHERE vr.name = $repo_name
                RETURN vr.name AS repo_name, v.id AS vuln_id, p.versions AS affected_versions
                SKIP {skip} LIMIT {self.batch_size}
                """

                results = list(session.run(query, {"repo_name": repo_name}))
                if not results:
                    break

                for record in results:
                    repo_name_db = record['repo_name']
                    vuln_id = record['vuln_id']
                    affected_versions = record['affected_versions']

                    if repo_name_db not in vuln_repo_map:
                        vuln_repo_map[repo_name_db] = {}

                    if vuln_id not in vuln_repo_map[repo_name_db]:
                        vuln_repo_map[repo_name_db][vuln_id] = []

                    # If p.versions is a single string, add it directly, else extend the list
                    if isinstance(affected_versions, list):
                        for version in affected_versions:
                            if version not in vuln_repo_map[repo_name_db][vuln_id]:
                                vuln_repo_map[repo_name_db][vuln_id].append(version)
                    else:
                        if affected_versions not in vuln_repo_map[repo_name_db][vuln_id]:
                            vuln_repo_map[repo_name_db][vuln_id].append(affected_versions)

                batch_size_here = len(results)
                processed_count += batch_size_here

                if processed_count % progress_interval < self.batch_size:
                    elapsed = time.time() - start_time
                    percent = (processed_count / total_count) * 100 if total_count > 0 else 0
                    rps = processed_count / elapsed if elapsed > 0 else 0
                    eta_seconds = (total_count - processed_count) / rps if rps > 0 else 0

                    print(f"Progress: {processed_count}/{total_count} ({percent:.1f}%) - "
                          f"Speed: {rps:.1f} records/sec - "
                          f"ETA: {datetime.fromtimestamp(time.time() + eta_seconds).strftime('%H:%M:%S')}")

                skip += batch_size_here
                if batch_size_here < self.batch_size:
                    break

        print(f"Completed processing {processed_count} records in {time.time() - start_time:.1f} seconds")
        total_vulns_found = sum(len(vulns) for vulns in vuln_repo_map.values())
        print(f"Found {total_vulns_found} unique vulnerabilities")

        return vuln_repo_map

    def build_minimal_hitting_sets_for_repo(self, repo_name="OSV"):
        """
        Task 3 solution:
        1) Retrieves all CVEs for the given repo -> cve_version_lists
        2) Solves the minimum hitting set problem.
        3) Stores or returns the minimal set of versions covering all CVEs.

        Return structure:
        {
          "OSV": {
            "minimal_cover": ["v1.1", "v1.2", ...],
            "min_cover_size": <int>
          }
        }
        """
        vuln_repo_map = self.get_vulnerability_repo_mapping_batched(repo_name=repo_name)
        if repo_name not in vuln_repo_map:
            print(f"No data found for repo {repo_name}.")
            return {}

        cve_dict = vuln_repo_map[repo_name]
        if not cve_dict:
            print(f"No CVEs found for repo {repo_name}.")
            return {}

        # Convert the dictionary into a list-of-lists
        cve_version_lists = list(cve_dict.values())
        print(f"DEBUG: cve_version_lists for {repo_name} -> {cve_version_lists}")

        # For demonstration, we generate recency stubs (replace or fetch real release dates)
        version_recency = self._get_version_recency_stub(cve_version_lists)

        # Solve the Minimum Hitting Set
        minimal_cover = find_minimum_hitting_set(cve_version_lists, version_recency)
        print(f"DEBUG: minimal_cover = {minimal_cover}")

        # Store the minimal cover in Neo4j (only if non-empty)
        self._store_minimal_cover_in_neo4j(repo_name, minimal_cover)

        return {
            repo_name: {
                "minimal_cover": minimal_cover,
                "min_cover_size": len(minimal_cover)
            }
        }

    def _store_minimal_cover_in_neo4j(self, repo_name, cover_list):
        print(f"Storing minimal cover for {repo_name}: {cover_list}")
        if not self._driver:
            print("No Neo4j driver available, skipping store.")
            return

        with self._driver.session() as session:
            query = """
            MATCH (r:VULN_REPO {name: $repo_name})
            SET r.minimal_versions = $versions
            RETURN r
            """
            print(f"DEBUG: Running Neo4j query to set minimal_versions on {repo_name}")
            session.run(query, repo_name=repo_name, versions=cover_list)
            print("DEBUG: minimal_versions successfully updated in Neo4j.")


    def _get_version_recency_stub(self, cve_version_lists):
        """
        Demonstration:
        Return { version: recency_score } with ascending numeric for sorting.
        """
        all_versions = set()
        for sub in cve_version_lists:
            all_versions.update(sub)

        version_list_sorted = sorted(all_versions)
        version_recency = {}
        score_start = 100
        for idx, ver in enumerate(version_list_sorted):
            version_recency[ver] = score_start + idx
        return version_recency

    def export_to_json(self, repo_name="OSV", filename=None):
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f'vulnerability_repo_map_{repo_name}_{timestamp}.json'

        print(f"Starting export to {filename}...")
        start_time = time.time()

        vuln_repo_map = self.get_vulnerability_repo_mapping_batched(repo_name)
        if not vuln_repo_map:
            print("No data to export.")
            return False

        with open(filename, 'w') as f:
            json.dump(vuln_repo_map, f, indent=2)

        print(f"Export completed in {time.time() - start_time:.1f} seconds")
        print(f"Exported vulnerability repo mapping to {filename}")
        return True

    def export_to_json_streaming(self, filename=None, progress_interval=10000):
        if not self._driver:
            print("Error: Not connected to Neo4j. Call connect() first.")
            return False

        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f'package_cve_versions_{timestamp}.json'

        print(f"Starting streaming export to {filename}...")
        start_time = time.time()

        total_count = self.get_vulnerability_count()
        print(f"Processing {total_count} vulnerability relationships...")

        processed_count = 0
        package_count = 0

        with open(filename, 'w') as f:
            f.write("{\n")  # Start the JSON object

            is_first_package = True
            with self._driver.session() as session:
                query = """
                    MATCH (v:Vulnerability)-[:AFFECTS]->(p:Package)
                    RETURN p.name AS package_name, p.ecosystem AS ecosystem, v.id AS vuln_id,
                           collect(p.versions) AS affected_versions
                    ORDER BY p.name, v.id
                """
                result = session.run(query)

                current_package = None
                is_first_vuln = True

                for record in result:
                    package_name = record['package_name']
                    ecosystem = record['ecosystem']
                    vuln_id = record['vuln_id']
                    all_versions = record['affected_versions']

                    affected_versions = []
                    for version_item in all_versions:
                        if isinstance(version_item, list):
                            affected_versions.extend(version_item)
                        else:
                            affected_versions.append(version_item)

                    unique_versions = list(set(affected_versions))

                    if package_name != current_package:
                        if current_package is not None:
                            f.write("\n        },\n")

                        if not is_first_package:
                            f.write(",\n")

                        f.write(f'        "{package_name}": {{\n')
                        f.write(f'            "ecosystem": "{ecosystem}",\n')
                        current_package = package_name
                        is_first_vuln = True
                        package_count += 1
                        is_first_package = False

                    if not is_first_vuln:
                        f.write(",\n")

                    f.write(f'            "{vuln_id}": {json.dumps(unique_versions, indent=12)}')
                    is_first_vuln = False
                    processed_count += 1

                    if processed_count % progress_interval == 0:
                        elapsed = time.time() - start_time
                        percent = (processed_count / total_count) * 100 if total_count > 0 else 0
                        rps = processed_count / elapsed if elapsed > 0 else 0
                        eta_seconds = (total_count - processed_count) / rps if rps > 0 else 0
                        print(f"Progress: {processed_count}/{total_count} ({percent:.1f}%) - "
                              f"Packages: {package_count} - "
                              f"Speed: {rps:.1f} records/sec - "
                              f"ETA: {datetime.fromtimestamp(time.time() + eta_seconds).strftime('%H:%M:%S')}")

                if current_package is not None:
                    f.write("\n        }\n")

                f.write("}\n")

        print(f"Completed processing {processed_count} records in {time.time() - start_time:.1f} seconds")
        print(f"Found {package_count} unique packages")
        print(f"Exported package CVE versions to {filename}")
        return True


def find_minimum_hitting_set(cve_version_lists, version_recency=None):
    """
    Returns a minimal set of versions that covers all CVEs in cve_version_lists.
    Breaks ties by choosing the set with maximum sum of recency scores.
    """
    # Edge-case: no CVEs at all
    if not cve_version_lists:
        return []
    for lst in cve_version_lists:
        if not lst:
            # A CVE has no versions => impossible
            return []

    # Collect all distinct versions
    all_versions = sorted({v for sublist in cve_version_lists for v in sublist})

    if version_recency is None:
        version_recency = {}
    rec = {v: version_recency.get(v, 0) for v in all_versions}

    model = cp_model.CpModel()
    x = {}
    for v in all_versions:
        x[v] = model.NewBoolVar(v)

    # Coverage constraints
    for cve_list in cve_version_lists:
        model.Add(sum(x[v] for v in cve_list) >= 1)

    cardinality = model.NewIntVar(0, len(all_versions), "cardinality")
    model.Add(cardinality == sum(x[v] for v in all_versions))
    model.Minimize(cardinality)

    solver = cp_model.CpSolver()
    status = solver.Solve(model)
    if status not in (cp_model.OPTIMAL, cp_model.FEASIBLE):
        return []

    min_cardinality = int(solver.ObjectiveValue())

    # If all rec=0, no need second pass
    if all(r == 0 for r in rec.values()):
        chosen_phase1 = []
        for v in all_versions:
            if solver.BooleanValue(x[v]):
                chosen_phase1.append(v)
        return chosen_phase1

    # Phase 2: fix cardinality, maximize sum of rec[v]
    model2 = cp_model.CpModel()
    x2 = {}
    for v in all_versions:
        x2[v] = model2.NewBoolVar(v)

    for cve_list in cve_version_lists:
        model2.Add(sum(x2[v] for v in cve_list) >= 1)

    cardinality2 = model2.NewIntVar(0, len(all_versions), "cardinality2")
    model2.Add(cardinality2 == sum(x2[v] for v in all_versions))
    model2.Add(cardinality2 == min_cardinality)

    recency_sum = model2.NewIntVar(0, 10**12, "recency_sum")
    model2.Add(recency_sum == sum(x2[v] * rec[v] for v in all_versions))
    model2.Maximize(recency_sum)

    solver2 = cp_model.CpSolver()
    status2 = solver2.Solve(model2)
    if status2 not in (cp_model.OPTIMAL, cp_model.FEASIBLE):
        # fallback: use phase 1 solution
        chosen_phase1 = []
        for v in all_versions:
            if solver.BooleanValue(x[v]):
                chosen_phase1.append(v)
        return chosen_phase1

    chosen_versions = []
    for v in all_versions:
        if solver2.BooleanValue(x2[v]):
            chosen_versions.append(v)

    return chosen_versions

def main():
    # Example driver code:
    mapper = VulnerabilityRepoMapper(batch_size=10000)
    try:
        if mapper.connect():
            print("Successfully connected to Neo4j database.")

            # Optional: Clean up duplicates (if desired)
            # mapper.deduplicate_osv_nodes()

            total_vulns = mapper.get_vulnerability_count()
            print(f"Found {total_vulns} vulnerabilities for repo 'OSV'.")

            # Build minimal hitting set
            result = mapper.build_minimal_hitting_sets_for_repo("OSV")
            print("Minimum Hitting Set Results:")
            print(json.dumps(result, indent=2))

            # Example: also do an export
            if total_vulns > 100000:
                print("Large dataset -> streaming export...")
                mapper.export_to_json_streaming()
            else:
                print("Standard batch processing export...")
                mapper.export_to_json()

        else:
            print("Failed to connect to Neo4j database.")

    except Exception as e:
        print(f"An error occurred: {e}")
    finally:
        mapper.close()
