from neo4j import GraphDatabase
import json
import time
from datetime import datetime
import logging
from osv.neo4j_connection import get_neo4j_driver
import math
from ortools.sat.python import cp_model
import re

class VulnerabilityRepoMapper:
    def __init__(self, batch_size=5000):
        self._driver = None
        self.batch_size = batch_size  # Number of records to process in each batch

    def connect(self):
        self._driver = get_neo4j_driver()
        return self._driver is not None

    def close(self):
        """Close the Neo4j connection if open"""
        if self._driver:
            self._driver.close()
            print("Neo4j connection closed.")

    def get_vulnerability_count(self):
        """Get the total count of vulnerabilities in the database for a specific repo"""
        if not self._driver:
            print("Error: Not connected to Neo4j. Call connect() first.")
            return 0

        with self._driver.session() as session:
            query = """
            MATCH (v:Vulnerability)-[:BELONGS_TO]->(vr:VULN_REPO)
            MATCH (v)-[:AFFECTS]->(p:Package)
            WHERE vr.name = 'OSV'
            RETURN COUNT(*) AS count
            """
            result = session.run(query)
            record = result.single()
            return record["count"] if record else 0

    def get_vulnerability_repo_mapping_batched(self, repo_name="OSV", progress_interval=10000):
        """
        Create a nested dictionary mapping vuln repositories to their vulnerabilities and affected versions.

        Output structure:
        {
          "OSV": {
            "CVE-1234": ["v1.0", "v1.2"],
            "CVE-5678": ["v1.1", "v1.3"],
            ...
          }
        }
        """
        if not self._driver:
            print("Error: Not connected to Neo4j. Call connect() first.")
            return {}

        total_count = self.get_vulnerability_count()
        print(f"Processing {total_count} vulnerabilities for repo '{repo_name}'...")

        vuln_repo_map = {}
        processed_count = 0
        start_time = time.time()

        with self._driver.session() as session:
            skip = 0

            while True:
                query = f"""
                MATCH (v:Vulnerability)-[:BELONGS_TO]->(vr:VULN_REPO)
                MATCH (v)-[:AFFECTS]->(p:Package)
                WHERE vr.name = $repo_name
                RETURN vr.name AS repo_name, v.id AS vuln_id, p.versions AS affected_versions
                SKIP {skip} LIMIT {self.batch_size}
                """

                results = list(session.run(query, {"repo_name": repo_name}))
                if not results:
                    break

                for record in results:
                    repo_name_db = record['repo_name']
                    vuln_id = record['vuln_id']
                    affected_versions = record['affected_versions']

                    if repo_name_db not in vuln_repo_map:
                        vuln_repo_map[repo_name_db] = {}

                    if vuln_id not in vuln_repo_map[repo_name_db]:
                        vuln_repo_map[repo_name_db][vuln_id] = []

                    # If p.versions is a single string, add it directly, else extend the list
                    if isinstance(affected_versions, list):
                        for version in affected_versions:
                            if version not in vuln_repo_map[repo_name_db][vuln_id]:
                                vuln_repo_map[repo_name_db][vuln_id].append(version)
                    else:
                        if affected_versions not in vuln_repo_map[repo_name_db][vuln_id]:
                            vuln_repo_map[repo_name_db][vuln_id].append(affected_versions)

                batch_size_here = len(results)
                processed_count += batch_size_here

                if processed_count % progress_interval < self.batch_size:
                    elapsed = time.time() - start_time
                    percent = (processed_count / total_count) * 100 if total_count > 0 else 0
                    rps = processed_count / elapsed if elapsed > 0 else 0
                    eta_seconds = (total_count - processed_count) / rps if rps > 0 else 0

                    print(f"Progress: {processed_count}/{total_count} ({percent:.1f}%) - "
                          f"Speed: {rps:.1f} records/sec - "
                          f"ETA: {datetime.fromtimestamp(time.time() + eta_seconds).strftime('%H:%M:%S')}")

                skip += batch_size_here
                if batch_size_here < self.batch_size:
                    break

        print(f"Completed processing {processed_count} records in {time.time() - start_time:.1f} seconds")
        total_vulns_found = sum(len(vulns) for vulns in vuln_repo_map.values())
        print(f"Found {total_vulns_found} unique vulnerabilities")

        return vuln_repo_map

    def build_minimal_hitting_sets_for_repo(self, repo_name="OSV"):
        """
        Builds a minimal hitting set for all CVEs in the specified repository.
        
        This implements Task 3 of Part 1:
        1) Gets all CVEs for the repository with their affected versions
        2) Converts the data into list-of-lists format for the solver
        3) Solves the minimum hitting set problem
        4) Stores the minimal version set in Neo4j for later reference
        
        Args:
            repo_name: Repository name to build the hitting set for (default: "OSV")
            
        Returns:
            Dictionary with repository name, minimal cover list, and size
            e.g., {"OSV": {"minimal_cover": ["v1.1", "v1.2"], "min_cover_size": 2}}
        """
        import time
        import logging

        logger = logging.getLogger(__name__)
        logger.info(f"Starting build_minimal_hitting_sets_for_repo for {repo_name}")

        start_time = time.time()

        print("DEBUG: Entering build_minimal_hitting_sets_for_repo")
        print(f"DEBUG: Repository name: {repo_name}")

        # Step 1: Get vulnerability-version mapping for the repository
        logger.info("Fetching vulnerability-version mapping from neo4j")
        vuln_repo_map = self.get_vulnerability_repo_mapping_batched(repo_name=repo_name)

        fetch_time = time.time() - start_time
        logger.info(f"Fetched vulnerability mapping in {fetch_time:.2f} seconds")
        
        print(f"DEBUG: Total repositories in map: {len(vuln_repo_map)}")
        print(f"DEBUG: Repositories found: {list(vuln_repo_map.keys())}")

        # Handle case where repository doesn't exist
        if repo_name not in vuln_repo_map:
            logger.warning(f"No data found for repo {repo_name}")
            return {repo_name: {"minimal_cover": [], "min_cover_size": 0}}
        
        # Get the CVE-to-versions dictionary for this repository
        cve_dict = vuln_repo_map[repo_name]
        cve_count = len(cve_dict)
        logger.info(f"Found {cve_count} CVEs for {repo_name}")
        print(f"DEBUG: Total CVEs for {repo_name}: {len(cve_dict)}")

        if not cve_dict:
            logger.warning(f"No CVEs found for repo {repo_name}")
            print(f"No CVEs found for repo {repo_name}.")
            return {repo_name: {"minimal_cover": [], "min_cover_size": 0}}
        
        # Sample and log information about the data to help with debugging
        if cve_count > 0:
            # Get a sample of CVEs for logging
            sample_size = min(5, cve_count)
            sample_keys = list(cve_dict.keys())[:sample_size]
            
            for key in sample_keys:
                versions = cve_dict[key]
                logger.info(f"Sample CVE {key}: {len(versions)} versions, example: {versions[:3]}...")

        # Step 2: Convert the dictionary into a list-of-lists format for the solver
        cve_version_lists = list(cve_dict.values())
        print(f"Found {len(cve_version_lists)} CVEs with version data for {repo_name}")
        
        # Step 3: Generate recency scores for the versions
        logger.info(f"Generating semantic version recency scores for all versions")
        version_recency_start = time.time()
        version_recency = self._get_semantic_version_recency(cve_version_lists)
        version_recency_time = time.time() - version_recency_start

        version_count = len(version_recency)
        logger.info(f"Generated recency scores for {version_count} unique versions in {version_recency_time:.2f} seconds")
        
        # Sample a few recency scores for debugging
        if version_count > 0:
            sample_versions = list(version_recency.keys())[:5]
            for v in sample_versions:
                logger.info(f"Version {v} has recency score {version_recency[v]}")
    
        # Step 3b: Solve the Minimum Hitting Set problem
        logger.info(f"Solving minimum hitting set problem for {cve_count} CVEs with {version_count} unique versions")
        solver_start_time = time.time()

        # Step 4: Solve the Minimum Hitting Set problem
        print(f"Solving minimum hitting set problem for {len(cve_version_lists)} CVEs...")
        minimal_cover = find_minimum_hitting_set(cve_version_lists, version_recency)
        
        solver_time = time.time() - solver_start_time
        logger.info(f"Minimum hitting set solver completed in {solver_time:.2f} seconds")
        logger.info(f"Found minimal cover with {len(minimal_cover)} versions")

        # For debugging, log a subset of the minimal cover if it's large
        if minimal_cover:
            log_limit = min(10, len(minimal_cover))
            logger.info(f"First {log_limit} versions in minimal cover: {minimal_cover[:log_limit]}")
    
        # Step 4: Verify the solution
        if minimal_cover:
            # Count how many CVEs are covered by this solution
            covered_cves = 0
            for vuln_id, versions in cve_dict.items():
                if any(v in minimal_cover for v in versions):
                    covered_cves += 1
        
            coverage_pct = (covered_cves / cve_count) * 100
            logger.info(f"Solution verification: covers {covered_cves}/{cve_count} CVEs ({coverage_pct:.1f}%)")
        
            # If we didn't cover 100%, something is wrong with our algorithm
            if covered_cves < cve_count:
                logger.error("ERROR: Solution does not cover all CVEs - algorithm error detected!")
    
        # Step 5: Store the minimal cover in Neo4j
        logger.info(f"Storing minimal cover in Neo4j for repo {repo_name}")
        store_start_time = time.time()
        self._store_minimal_cover_in_neo4j(repo_name, minimal_cover)
        store_time = time.time() - store_start_time
        logger.info(f"Storing minimal cover in Neo4j took {store_time:.2f} seconds")
    
        # Return the result
        total_time = time.time() - start_time
        logger.info(f"Total processing time for build_minimal_hitting_sets_for_repo: {total_time:.2f} seconds")
    
        return {
            repo_name: {
                "minimal_cover": minimal_cover,
                "min_cover_size": len(minimal_cover)
            }
        }
    
    def build_minimal_hitting_sets_per_package(self, repo_name="OSV", filename=None):
        """
        Build minimal hitting sets for each package in the repository.
        
        Args:
            repo_name: Repository name (default: "OSV")
            filename: Output file name for the JSON (optional)
            
        Returns:
            Dictionary containing package-organized data with minimal hitting sets
        """
        import time
        import logging
        
        logger = logging.getLogger(__name__)
        logger.info(f"Starting build_minimal_hitting_sets_per_package for {repo_name}")
        
        start_time = time.time()
        
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f'package_minimal_sets_{repo_name}_{timestamp}.json'
        
        # Organize data by package
        print(f"Fetching vulnerability data by package for repo '{repo_name}'...")
        
        # Initialize results structure
        results = {}
        package_count = 0
        processed_cves = 0
        
        # Query data organized by package
        if not self._driver:
            print("Error: Not connected to Neo4j. Call connect() first.")
            return None
        
        try:
            with self._driver.session() as session:
                query = """
                    MATCH (v:Vulnerability)-[:BELONGS_TO]->(vr:VULN_REPO)
                    MATCH (v)-[:AFFECTS]->(p:Package)
                    WHERE vr.name = $repo_name
                    RETURN p.name AS package_name, p.ecosystem AS ecosystem, 
                        v.id AS vuln_id, p.versions AS affected_versions
                    ORDER BY p.name, v.id
                """
                
                # First, organize data by package
                packages = {}
                for record in session.run(query, {"repo_name": repo_name}):
                    package_name = record['package_name']
                    ecosystem = record['ecosystem']
                    vuln_id = record['vuln_id']
                    affected_versions = record['affected_versions']
                    
                    # Process versions
                    versions_list = []
                    if isinstance(affected_versions, list):
                        for version in affected_versions:
                            if version not in versions_list:
                                versions_list.append(version)
                    elif affected_versions is not None:
                        if affected_versions not in versions_list:
                            versions_list.append(affected_versions)
                    
                    # Skip if no versions
                    if not versions_list:
                        continue
                    
                    processed_cves += 1
                    
                    # Initialize package entry if needed
                    if package_name not in packages:
                        packages[package_name] = {
                            "ecosystem": ecosystem,
                            "vulnerabilities": {}
                        }
                        package_count += 1
                    
                    # Add vulnerability data
                    packages[package_name]["vulnerabilities"][vuln_id] = versions_list
            
            print(f"Found {package_count} packages with {processed_cves} CVEs")
            
            # Now process each package to find its minimum hitting set
            processed = 0
            results = {}
            
            for package_name, package_data in packages.items():
                print(f"Processing package {processed+1}/{package_count}: {package_name}")
                
                # Extract version lists for this package
                vulns = package_data["vulnerabilities"]
                version_lists = list(vulns.values())
                
                # Skip if no valid data
                if not version_lists:
                    continue
                
                # Generate recency scores for this package
                version_recency = self._get_semantic_version_recency(version_lists)
                
                # Find minimum hitting set for this package
                start_solve = time.time()
                minimal_cover = find_minimum_hitting_set(version_lists, version_recency)
                solve_time = time.time() - start_solve
                
                # Store in results
                if package_name not in results:
                    results[package_name] = {
                        "ecosystem": package_data["ecosystem"],
                        "vulnerabilities": {}
                    }
                
                # Store vulnerabilities
                results[package_name]["vulnerabilities"] = vulns
                
                # Add minimal cover
                results[package_name]["minimal_versions"] = minimal_cover
                results[package_name]["min_cover_size"] = len(minimal_cover)
                
                processed += 1
                
                # Simple progress reporting
                if processed % 10 == 0 or processed == package_count:
                    elapsed = time.time() - start_time
                    print(f"Progress: {processed}/{package_count} packages ({processed/package_count*100:.1f}%) - "
                        f"Elapsed: {elapsed:.1f}s")
            
            # Write results to file
            try:
                with open(filename, 'w') as f:
                    json.dump(results, f, indent=2)
                print(f"Exported package-based minimal hitting sets to {filename}")
            except Exception as e:
                print(f"Error writing to file: {e}")
            
            total_time = time.time() - start_time
            print(f"Completed in {total_time:.1f} seconds")
            return results
        
        except Exception as e:
            print(f"Error building minimal hitting sets per package: {e}")
            import traceback
            traceback.print_exc()
            return None
        

    def _store_minimal_cover_in_neo4j(self, repo_name, cover_list):
        """
        Store the minimal version cover in Neo4j for a given repository.
        
        Args:
            repo_name: Name of the repository (e.g., "OSV")
            cover_list: List of versions making up the minimal cover
            
        Returns:
            None
        """
        if not self._driver:
            print("Error: Not connected to Neo4j. Call connect() first.")
            return
        
        if not cover_list:
            print(f"Warning: Empty minimal cover for {repo_name}, skipping Neo4j update.")
            return
        
        print(f"Storing minimal cover with {len(cover_list)} versions for repo {repo_name} in Neo4j...")
        
        try:
            with self._driver.session() as session:
                # Update or create the VULN_REPO node with the minimal_versions property
                query = """
                MERGE (r:VULN_REPO {name: $repo_name})
                SET r.minimal_versions = $versions,
                    r.minimal_versions_count = size($versions),
                    r.minimal_versions_updated = datetime()
                RETURN r.name as repo_name, r.minimal_versions_count as count
                """
                
                result = session.run(
                    query, 
                    repo_name=repo_name, 
                    versions=cover_list
                )
                
                # Get the first record from the result to confirm success
                record = result.single()
                if record:
                    print(f"Successfully updated {record['repo_name']} with {record['count']} minimal versions in Neo4j.")
                else:
                    print(f"Warning: Query executed but no confirmation returned for {repo_name}.")
        
        except Exception as e:
            print(f"Error storing minimal cover in Neo4j: {e}")


    def _get_semantic_version_recency(self, cve_version_lists):
        """
        Create a recency score mapping for all versions that properly handles
        semantic versioning patterns (e.g., v1.2.3, 1.2.3-alpha, etc.).
        
        Args:
            cve_version_lists: List of lists containing affected versions for each CVE
            
        Returns:
            Dictionary mapping each version to a recency score (higher = more recent)
        """
        import re
        from datetime import datetime
        
        # Collect all unique versions
        all_versions = set()
        for sublist in cve_version_lists:
            all_versions.update(sublist)
        
        # Define regex patterns for version strings
        # Pattern for semantic versions like v1.2.3 or 1.2.3-alpha
        semver_pattern = re.compile(r'^v?(\d+)(?:[.-](\d+))?(?:[.-](\d+))?(?:[.-](\d+))?(?:[.-]([a-zA-Z0-9-]+))?$')
        # Pattern for date-based versions like 2021-03-15 or 20210315
        date_pattern = re.compile(r'^(\d{4})-?(\d{2})-?(\d{2})$')
        
        # Store parsed version components
        version_components = {}
        
        # Process each version string
        for version in all_versions:
            # Try semver pattern first
            semver_match = semver_pattern.match(version)
            if semver_match:
                # Extract numeric components, fill with zeros if missing
                components = []
                for i in range(1, 5):  # Groups 1-4 are version numbers
                    group = semver_match.group(i)
                    components.append(int(group) if group and group.isdigit() else 0)
                
                # Check for suffix (alpha, beta, etc.) that should reduce priority
                suffix = semver_match.group(5)
                suffix_penalty = 0
                if suffix:
                    # Pre-releases should be ranked lower than final releases
                    suffix_lower = suffix.lower()
                    if 'alpha' in suffix_lower:
                        suffix_penalty = -30
                    elif 'beta' in suffix_lower:
                        suffix_penalty = -20
                    elif 'rc' in suffix_lower or 'pre' in suffix_lower:
                        suffix_penalty = -10
                
                # Add suffix penalty to last component
                components[3] += suffix_penalty
                
                version_components[version] = components
                continue
            
            # Try date pattern
            date_match = date_pattern.match(version)
            if date_match:
                try:
                    year, month, day = map(int, date_match.groups())
                    # Convert date to numeric value (days since epoch)
                    date_obj = datetime(year, month, day)
                    epoch = datetime(1970, 1, 1)
                    days = (date_obj - epoch).days
                    # Use high first component to rank dates above regular versions
                    version_components[version] = [10000, days, 0, 0]
                    continue
                except (ValueError, OverflowError):
                    # Invalid date, fall through to default handling
                    pass
            
            # Default handling for non-matched versions
            # Convert to string and use character codes as components
            version_components[version] = [0, 0, 0, 0]  # Lowest priority
        
        # Sort versions based on components
        sorted_versions = sorted(version_components.items(), key=lambda x: tuple(x[1]))
        
        # Create recency scores (higher = more recent)
        version_recency = {}
        base_score = 100
        for i, (version, _) in enumerate(sorted_versions):
            version_recency[version] = base_score + i
        
        return version_recency

    def export_to_json(self, repo_name="OSV", filename=None):
        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f'vulnerability_repo_map_{repo_name}_{timestamp}.json'

        print(f"Starting export to {filename}...")
        start_time = time.time()

        vuln_repo_map = self.get_vulnerability_repo_mapping_batched(repo_name)
        if not vuln_repo_map:
            print("No data to export.")
            return False

        with open(filename, 'w') as f:
            json.dump(vuln_repo_map, f, indent=2)

        print(f"Export completed in {time.time() - start_time:.1f} seconds")
        print(f"Exported vulnerability repo mapping to {filename}")
        return True

    def export_to_json_streaming(self, filename=None, progress_interval=10000):
        if not self._driver:
            print("Error: Not connected to Neo4j. Call connect() first.")
            return False

        if filename is None:
            timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
            filename = f'package_cve_versions_{timestamp}.json'

        print(f"Starting streaming export to {filename}...")
        start_time = time.time()

        total_count = self.get_vulnerability_count()
        print(f"Processing {total_count} vulnerability relationships...")

        processed_count = 0
        package_count = 0

        with open(filename, 'w') as f:
            f.write("{\n")  # Start the JSON object

            is_first_package = True
            with self._driver.session() as session:
                query = """
                    MATCH (v:Vulnerability)-[:AFFECTS]->(p:Package)
                    RETURN p.name AS package_name, p.ecosystem AS ecosystem, v.id AS vuln_id,
                           collect(p.versions) AS affected_versions
                    ORDER BY p.name, v.id
                """
                result = session.run(query)

                current_package = None
                is_first_vuln = True

                for record in result:
                    package_name = record['package_name']
                    ecosystem = record['ecosystem']
                    vuln_id = record['vuln_id']
                    all_versions = record['affected_versions']

                    affected_versions = []
                    for version_item in all_versions:
                        if isinstance(version_item, list):
                            affected_versions.extend(version_item)
                        else:
                            affected_versions.append(version_item)

                    unique_versions = list(set(affected_versions))

                    if package_name != current_package:
                        if current_package is not None:
                            f.write("\n        },\n")

                        if not is_first_package:
                            f.write(",\n")

                        f.write(f'        "{package_name}": {{\n')
                        f.write(f'            "ecosystem": "{ecosystem}",\n')
                        current_package = package_name
                        is_first_vuln = True
                        package_count += 1
                        is_first_package = False

                    if not is_first_vuln:
                        f.write(",\n")

                    f.write(f'            "{vuln_id}": {json.dumps(unique_versions, indent=12)}')
                    is_first_vuln = False
                    processed_count += 1

                    if processed_count % progress_interval == 0:
                        elapsed = time.time() - start_time
                        percent = (processed_count / total_count) * 100 if total_count > 0 else 0
                        rps = processed_count / elapsed if elapsed > 0 else 0
                        eta_seconds = (total_count - processed_count) / rps if rps > 0 else 0
                        print(f"Progress: {processed_count}/{total_count} ({percent:.1f}%) - "
                              f"Packages: {package_count} - "
                              f"Speed: {rps:.1f} records/sec - "
                              f"ETA: {datetime.fromtimestamp(time.time() + eta_seconds).strftime('%H:%M:%S')}")

                if current_package is not None:
                    f.write("\n        }\n")

                f.write("}\n")

        print(f"Completed processing {processed_count} records in {time.time() - start_time:.1f} seconds")
        print(f"Found {package_count} unique packages")
        print(f"Exported package CVE versions to {filename}")
        return True

def find_minimum_hitting_set(cve_version_lists, version_recency=None):
    """
    Returns a minimal set of versions that covers all CVEs in cve_version_lists.
    Breaks ties by choosing the set with maximum sum of recency scores.
    
    Args:
        cve_version_lists: List of lists where each sublist contains versions affected by a specific CVE
        version_recency: Dictionary mapping versions to their recency scores (higher = more recent)
                         If None, will not prioritize by recency
    
    Returns:
        List of versions forming the minimum hitting set
    """
    from ortools.sat.python import cp_model
    import time
    import logging

    # sets up logging

    logger = logging.getLogger(__name__)

    start_time = time.time()

    # input validation
    if not isinstance(cve_version_lists, list):
        logger.error(f"Input must be a list of lists, got {type(cve_version_lists)}")
        return []

    # Filter out empty version lists first
    non_empty_lists = [lst for lst in cve_version_lists if lst]
    if len(non_empty_lists) != len(cve_version_lists):
        filtered_out = len(cve_version_lists) - len(non_empty_lists)
        logger.warning(f"Filtered out {filtered_out} empty CVE version lists")

    # Edge-case handling: empty input or any empty set makes hitting impossible
    if not non_empty_lists:
        logger.warning("No valid CVE version lists to process")
        return []
    
    # Get all unique versions across all CVEs
    all_versions = sorted(set(v for sublist in non_empty_lists for v in sublist))
    if not all_versions:
        logger.warning("No versions found in the input lists")
        return []
    
    logger.info(f"Processing {len(non_empty_lists)} CVEs with {len(all_versions)} unique versions")

    # If version_recency is not provided, create a default one with all zeros
    if version_recency is None:
        version_recency = {}
    
    # Create a dictionary with recency scores, defaulting to 0 if not specified
    rec = {v: version_recency.get(v, 0) for v in all_versions}
    
    logger.info("Phase 1: Finding minimum hitting set cardinality")
    phase1_start = time.time()

    try: 
        # Set up the CP-SAT model
        model = cp_model.CpModel()
        
        # Create boolean variables for each version
        x = {}
        for v in all_versions:
            x[v] = model.NewBoolVar(v)  # Boolean variable indicating if version v is in the solution

        # Add constraints to ensure each CVE is covered by at least one version
        for i, cve_list in enumerate(non_empty_lists):
            model.Add(sum(x[v] for v in cve_list) >= 1)
    
        # Create a variable for the total number of versions in the solution
        cardinality = model.NewIntVar(0, len(all_versions), "cardinality")
        model.Add(cardinality == sum(x[v] for v in all_versions))

        # Minimize the number of versions
        model.Minimize(cardinality)
    
        # set up solver with timeout to avoid hanging on very large problems
        solver = cp_model.CpSolver()
        solver.parameters.max_time_in_seconds = 300  # 5 min timeout

        # Solve phase 1
        status = solver.Solve(model)

        # Handle solver status
        if status == cp_model.OPTIMAL:
            logger.info("Found optimal solution in phase 1")
        elif status == cp_model.FEASIBLE:
            logger.info("Found feasible (but possibly not optimal) solution in phase 1")
        else:
            logger.warning(f"Phase 1 solver status: {status} - no solution found")
            return []
        
        # Get the minimum cardinality value
        min_cardinality = int(solver.ObjectiveValue())
        logger.info(f"Minimum hitting set cardinality: {min_cardinality}")

         # If all recency scores are the same (or zero), we can skip phase 2
        if len(set(rec.values())) <= 1:
            logger.info("All versions have same recency score - skipping phase 2")
            chosen_versions = []
            for v in all_versions:
                if solver.BooleanValue(x[v]):
                    chosen_versions.append(v)

            phase1_time = time.time() - phase1_start
            logger.info(f"Phase 1 completed in {phase1_time:.2f} seconds")
            return chosen_versions
        
        # Phase 2: fix cardinality to min value and maximize recency
        logger.info("Phase 2: Maximizing recency while maintaining minimum cardinality")
        phase2_start = time.time()

        model2 = cp_model.CpModel()
        x2 = {}
        for v in all_versions:
            x2[v] = model2.NewBoolVar(v)

        # Copy constraints from phase 1
        for i, cve_list in enumerate(non_empty_lists):
            model2.Add(sum(x2[v] for v in cve_list) >= 1)

         # Fix cardinality to the minimum found in phase 1
        cardinality2 = model2.NewIntVar(0, len(all_versions), "cardinality2")
        model2.Add(cardinality2 == sum(x2[v] for v in all_versions))
        model2.Add(cardinality2 == min_cardinality)
        
        # We need to scale recency scores to avoid floating-point issues in the solver
        # Find the max recency value to help with scaling
        max_recency = max(rec.values()) if rec else 1


         # Multiply by 1000 and convert to int to handle decimal values
        # This creates a scaled version of the objective function
        scale_factor = 1000
        scaled_terms = []
        for v in all_versions:
            # Scale and convert to int
            scaled_value = int(rec[v] * scale_factor)
            # Only add non-zero terms
            if scaled_value != 0:
                scaled_terms.append(x2[v] * scaled_value)

        # Create objective variable with appropriate bounds
        max_possible_sum = len(all_versions) * max_recency * scale_factor
        recency_sum = model2.NewIntVar(0, max_possible_sum, "recency_sum")

        # If we have scaled terms, use them; otherwise use a default sum of zeros
        if scaled_terms:
            model2.Add(recency_sum == sum(scaled_terms))
        else:
            model2.Add(recency_sum == 0)

        # Set objective to maximize recency
        model2.Maximize(recency_sum)
        
        # Solve phase 2
        solver2 = cp_model.CpSolver()
        solver2.parameters.max_time_in_seconds = 300  # 5-minute timeout
        status2 = solver2.Solve(model2)
        
        # Handle solver status
        if status2 == cp_model.OPTIMAL:
            logger.info("Found optimal solution in phase 2")
        elif status2 == cp_model.FEASIBLE:
            logger.info("Found feasible (but possibly not optimal) solution in phase 2")
        else:
            logger.warning(f"Phase 2 solver status: {status2} - falling back to phase 1 solution")
            # Fall back to phase 1 solution
            chosen_versions = []
            for v in all_versions:
                if solver.BooleanValue(x[v]):
                    chosen_versions.append(v)
            
            total_time = time.time() - start_time
            logger.info(f"Total solving time: {total_time:.2f} seconds")
            return chosen_versions
        
        # Build solution from phase 2
        chosen_versions = []
        for v in all_versions:
            if solver2.BooleanValue(x2[v]):
                chosen_versions.append(v)
        
        phase2_time = time.time() - phase2_start
        total_time = time.time() - start_time
        logger.info(f"Phase 2 completed in {phase2_time:.2f} seconds")
        logger.info(f"Total solving time: {total_time:.2f} seconds")
        
        # Verify the solution is valid
        covered_cves = 0
        for cve_list in non_empty_lists:
            if any(v in chosen_versions for v in cve_list):
                covered_cves += 1
            else:
                logger.error(f"Solution verification failed: CVE list {cve_list} not covered by {chosen_versions}")
                
        coverage_pct = (covered_cves / len(non_empty_lists)) * 100
        logger.info(f"Solution covers {covered_cves}/{len(non_empty_lists)} CVEs ({coverage_pct:.1f}%)")
        
        return chosen_versions
    
    except Exception as e:
        logger.error(f"Error solving minimum hitting set: {e}")
        return []

def main():
    mapper = VulnerabilityRepoMapper(batch_size=10000)
    try:
        if mapper.connect():
            print("Successfully connected to Neo4j database.")

            # Optional: Clean up duplicates (if desired)
            # mapper.deduplicate_osv_nodes()

            total_vulns = mapper.get_vulnerability_count()
            print(f"Found {total_vulns} vulnerabilities for repo 'OSV'.")

            # Build minimal hitting sets per package
            print("Finding minimal hitting sets per package...")
            result = mapper.build_minimal_hitting_sets_per_package("OSV")
            print("Completed per-package minimal hitting sets")

            print(f"Results saved to file with data for {len(result) if result else 0} packages")

        else:
            print("Failed to connect to Neo4j database.")

    except Exception as e:
        print(f"An error occurred: {e}")
    finally:
        mapper.close()
